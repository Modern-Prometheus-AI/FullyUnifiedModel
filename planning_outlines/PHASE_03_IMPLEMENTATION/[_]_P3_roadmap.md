# FUM Superintelligence Roadmap

## Phase 3: Implementation
- [ ] **Achievement Criteria**: Build and test FUM’s components to achieve superintelligence with full autonomy on your workstation, incorporating advanced stability, learning, and scaling mechanisms detailed in `How_It_Works`.
- [ ] **Success Statement**: I will know I achieved success on this phase when FUM runs at 7M neurons (or higher) with >95% accuracy on target benchmarks (Ref: 1.A.7), <5s inference, constant learning, dynamic memory, robust stability (SOC, plasticity), and sovereign goal-setting, processing 300 multimodal inputs without prompts.

### Task 1: Code Core Components & Learning Mechanisms
- [ ] **Success Statement**: I will know I achieved success on this task when LIF, enhanced STDP (diversity, STC, exploration, scaling), enhanced SIE (cluster rewards, gaming prevention, ethical alignment), Adaptive Clustering, Knowledge Graph mechanisms (hints, persistence, pathology detection), and Structural Plasticity enable stable, adaptive, efficient, and biologically plausible learning, validated through unit and integration tests.
- **Current Status**: *Incomplete*—Basic LIF, STDP, SIE, KG Hints implemented. Advanced features, clustering, structural plasticity, stability mechanisms, and scaling pending.

- [ ] **Step 1: Implement LIF Neuron Logic**
  - [ ] **Validation Check**: LIF neurons fire correctly, incorporating heterogeneity and intrinsic plasticity according to design (Ref: 2A.5, 2A.6).
  - [ ] **Success Statement**: I will know I achieved success on this step when 7M neurons process spikes in <3s with stable firing rates maintained by intrinsic plasticity.
  - [ ] **Actionable Substeps**:
    - [x] Code LIF dynamics (`V_i(t) = V_i(t-1) + I_i(t) - (V_i(t-1) / tau_i) * dt`), firing (`V > v_th`), and reset (`V = v_reset`) in `unified_neuron.py` (Ref: 2A.3.i, 2A.4).
    - [ ] Define simulation timestep `dt = 1ms` (Ref: 2A.3.ii).
    - [ ] Implement neuron parameter heterogeneity at initialization (Ref: 2A.5.i):
        - [ ] Initialize `tau_i` from `N(20ms, 2ms^2)` (`torch.normal(mean=20.0, std=2.0)`).
        - [ ] Initialize `v_th_i` from `N(-55mV, 2mV^2)` (`torch.normal(mean=-55.0, std=2.0)`).
        - [ ] Set `v_reset = -70mV` (fixed).
    - [ ] Implement intrinsic plasticity rules (Ref: 2A.6.i):
        - [ ] Define target firing rate range (0.1–0.5 Hz).
        - [ ] Calculate neuron firing rate `rate_i` over 50 timesteps.
        - [ ] Apply adjustment logic every 50 timesteps:
            - [ ] If `rate_i > 0.5 Hz`: `v_th += 0.1`, `tau -= 0.1`.
            - [ ] If `rate_i < 0.1 Hz`: `v_th -= 0.1`, `tau += 0.1`.
        - [ ] Clamp parameters to bounds: `v_th` in [-60mV, -50mV], `tau` in [15ms, 25ms].
    - [ ] Integrate `neuron_kernel.hip` (LIF kernel) for LIF update acceleration (Ref: 2A.7, 2E).
        - [ ] Ensure kernel uses `float16` tensors for computation (Ref: 2A.7.i).
        - [ ] Ensure kernel records `spike_history` as `uint8` (Ref: 2A.7.ii).
        - [ ] Verify kernel does *not* compute STDP/traces (Ref: 2A.7.ii).
    - [x] Test: Unit test basic LIF dynamics (`test_neuron.py` or similar).
    - [ ] Test: Unit test heterogeneity initialization (verify distributions).
    - [ ] Test: Unit test intrinsic plasticity rules (verify adjustments and bounds).
    - [ ] Test: Unit test HIP kernel integration path (data marshalling, functional equivalence).
    - [ ] Scale to 7M—verify <3s inference and stable firing rates via logs.
  - [ ] **Test: Unit test Step 1 functionalities.**

- [ ] **Step 2: Implement Enhanced STDP Learning**
  - [ ] **Validation Check**: Weights update dynamically, incorporating diversity, STC, exploration, and scaling mechanisms stably and effectively (Ref: 2B).
  - [ ] **Success Statement**: I will know I achieved success on this step when STDP adjusts 7M neuron weights in <1s, demonstrating stable, flexible, biologically plausible learning with effective credit assignment and exploration.
  - [ ] **Actionable Substeps**:
    - [x] Code basic STDP tensor logic & integrate (`resonance_enhanced_stdp.py`, `unified_neuron.py`) (Ref: 2B.2, B.3, B.5).
        - [ ] Implement excitatory rule: `Δw = A_+ * exp(-Δt / τ_+)` if `Δt > 0`, `-A_- * exp(Δt / τ_-)` if `Δt < 0` (Ref: 2B.2.i).
        - [ ] Implement inhibitory rule: `Δw = -A_+ * exp(-Δt / τ_+)` if `Δt > 0`, `A_- * exp(Δt / τ_-)` if `Δt < 0` (Ref: 2B.3.ii).
        - [ ] Implement check for pre-synaptic neuron type (`is_inhibitory[i]`) to apply correct rule (Ref: 2B.3.iii).
        - [ ] Define base parameters: `A_+=0.1`, `A_-=0.12`, `τ_+=20ms`, `τ_-=20ms` (Ref: 2B.4.i).
        - [ ] Implement weight clamping `w_ij` to `[-1, 1]` (Ref: 2B.4.ii).
    - [ ] Implement constrained biological diversity for STDP parameters (Ref: 2B.4.iv):
        - [ ] Initialize base variability per synapse/cluster: `A_+_base = 0.1 + 0.05 * rand()`, `τ_+ = 20ms + 5ms * rand()`.
        - [ ] Enforce biological range constraints: Clamp `A_+_base` to `[0.05, 0.15]`, `τ_+` to `[15ms, 25ms]`.
        - [ ] Implement SIE modulation: Calculate `A_+ = A_+_base * (cluster_reward[c] / max_reward)`.
        - [ ] Implement rate dependency: Calculate `A_+ *= spike_rate[i] / target_rate`.
        - [ ] *Optional:* Implement synapse-specific variability based on location: `A_+[i,j] = ... * (1 + 0.5 * synapse_location[i,j])`.
        - [ ] *Optional:* Implement neuron-type dependency: `τ_+[i] = 20ms` if E else `15ms`.
    - [ ] Implement Synaptic Tagging and Capture (STC) analogue for eligibility traces (Ref: 2B.5.ix):
        - [ ] Implement tagging logic: Set `tag_ij(t) = 1` if `Δw_ij(t) > 0.05`.
        - [ ] Implement long-term consolidation: Check `sum(tag_history_ij[-100k:]) == 100k`, if true `w_ij += 0.1`.
        - [ ] Modify trace update to incorporate tag: `e_ij(t) = ... + Δw_ij(t) * tag_ij(t)`.
        - [ ] Integrate STC logic with inhibitory feedback for interference prevention.
    - [ ] Implement standard eligibility traces (Ref: 2B.5):
        - [ ] Implement update rule: `e_ij(t) = γ * e_ij(t-1) + Δw_ij(t)` (Ref: 2B.5.ii).
        - [ ] Define base decay `γ = 0.95` (Ref: 2B.5.iii).
        - [ ] *Optional:* Implement variable decay based on activity: `γ = 0.95 + 0.04 * (1 - mean(rates) / 0.5)` (Ref: 2B.5.iii).
        - [ ] Store traces as sparse FP16 tensors on MI100 GPU (Ref: 2B.5.v).
        - [ ] Implement trace interference prevention mechanisms (Ref: 2B.5.viii):
            - [ ] Detect task boundaries (cluster transitions or `similarity < 0.5`).
            - [ ] Implement trace reset (`e_ij = 0`) on hard boundary or decay acceleration (`γ = 0.9`) on low similarity.
            - [ ] *Optional:* Implement task-specific traces (`e_ij[task_id]`).
            - [ ] Implement reward gating: Scale trace contribution `e_ij[c] *= 0.5` if `avg_reward[c] < 0.5`.
    - [ ] Implement basic exploration mechanisms (Ref: 2B.8.iii):
        - [ ] Implement Stochastic STDP: Add noise `0.01 * randn()` to `Δw_ij`.
        - [ ] Implement Neutral Drift: Add noise `0.005 * randn()` to `Δw_ij` if `reward > 0.9` and `variance < 0.05 Hz`.
    - [ ] Implement Synaptic Scaling with interaction logic (Ref: 2B.7.ii):
        - [ ] Implement scaling calculation (every 1000 steps): Calculate `total_exc = sum(w)`, `scale_factor = 1 / total_exc` if `total_exc > 1`. Apply `w *= scale_factor`.
        - [ ] Ensure timing: Apply scaling *after* STDP/SIE updates within the cycle.
        - [ ] Implement gating/protection logic: Check reward stability, synapse update recency, weight strength (`w < 0.8`), cluster reward before applying scaling.
    - [ ] Implement jitter mitigation / temporal noise filtering:
        - [ ] Implement adaptive STDP window based on max latency: `τ_+ = 20 + max_latency` (Ref: 5D.2.vii, 2B.2.iii).
        - [ ] Implement temporal noise filter: Apply low-pass filter `mean(spike_train[t-3:t+1])` to spike trains before STDP calc (Ref: 2B.2.vi, 2B.7.iv).
        - [ ] Implement latency-aware STDP scaling: Scale `Δw_ij *= (1 - latency_error / max_latency)` (Ref: 5D.2.vii).
        - [ ] Implement Kalman filter for spike timing refinement: `t_refined = Kalman(t_adjusted, latency_error)` (Ref: 5D.2.vii).
        - [ ] Implement temporal integration in STDP calculation: Average spike timings over ~5ms window (Ref: 5D.2.vii).
    - [ ] Implement fail-graceful STDP updates: Log errors, skip faulty `Δw_ij` updates (Ref: 2B.6.ii).
    - [ ] Integrate `neural_sheath.cpp` kernel for STDP calculation/update acceleration (Ref: 2B.6, 2E).
    - [x] Test: Unit test basic STDP tensor logic (Excitatory & Inhibitory rules, params, clamping).
    - [ ] Test: Unit test STDP diversity mechanisms (variability init, constraints, SIE mod, rate dep, optional synapse/type).
    - [ ] Test: Unit test STC analogue implementation (tagging, consolidation calc, trace mod, inhibition interaction).
    - [ ] Test: Unit test eligibility trace logic (standard update, base decay, optional variable decay, storage, interference prevention logic).
    - [ ] Test: Unit test exploration mechanisms (stochastic STDP noise, neutral drift logic).
    - [ ] Test: Unit test Synaptic Scaling implementation (calculation, timing logic, gating/protection logic).
    - [ ] Test: Unit test jitter mitigation techniques (adaptive window calc, filter application, latency scaling calc, Kalman filter impl, temporal integration).
    - [ ] Test: Unit test fail-graceful update logic (error logging, skipping).
    - [ ] Test: Unit test C++ kernel integration path.
    - [ ] Test: Validate STDP stability across parameter variations (target 95% stability) (Ref: 2B.4.iii).
    - [ ] Scale to 7M—verify <1s update time via logs.
  - [ ] **Test: Unit test Step 2 functionalities.**

- [ ] **Step 3: Implement Adaptive Clustering**
  - [ ] **Validation Check**: Clustering dynamically identifies functional domains, adapts k appropriately, handles edge cases, and provides stable state representations (Ref: 2F).
  - [ ] **Success Statement**: I will know I achieved success on this step when clustering runs efficiently and provides stable state representations for SIE.
  - [ ] **Actionable Substeps**:
    - [ ] Implement `AdaptiveClustering` class (`clustering.py`?) using k-means (`torch.kmeans`) on firing rates (Ref: 2F.1.ii).
    - [ ] Implement dynamic k selection using Silhouette Score (Ref: 2F.2.i):
        - [ ] Define range `[k_min, max_k]` (`k_min=8`, `max_k=min(N//50, k_min*2)`).
        - [ ] Loop through k range, run k-means, compute silhouette score for each k.
        - [ ] Select `best_k = argmax(scores)`.
        - [ ] Set final `k = max(best_k, k_min)`.
    - [ ] Implement handling for edge cases (Ref: 2F.4):
        - [ ] If dynamic selection yields `k < k_min`, set `k = k_min` (rerun if needed) (Ref: 2F.4.i).
        - [ ] If `num_inputs[c] == 0` after 1000 steps, set `avg_reward[c] = 0` and log (Ref: 2F.4.ii).
    - [ ] Implement mechanisms for handling novel domains/inputs (Ref: 2F.5.ii):
        - [ ] Implement novelty-driven bifurcation: Check if `novelty > 0.9` and `max_similarity < 0.5`, if true increment `k` and re-cluster.
        - [ ] *Optional:* Implement temporary holding cluster assignment for novel inputs.
        - [ ] Implement misattribution handling: If novel input classified to cluster `c` yields `total_reward < 0`, trigger reassignment or bifurcation.
    - [ ] Implement mechanisms for mitigating clustering instability for TD learning (Ref: 2F.5.iii):
        - [ ] Implement soft clustering: Calculate probabilities `cluster_probs = softmax(-distances)`.
        - [ ] Use probabilities to weight `V_states` updates: `V_states[idx] += α * TD * cluster_probs[idx]`.
        - [ ] Implement stable dynamic k adjustment: Adjust `k` incrementally (e.g., `k += 10` if `functional_coherence[c] < 0.8`).
        - [ ] Initialize `V_states` for new clusters based on average of split clusters: `V_states[new_idx] = mean(V_states[old_indices])`.
    - [ ] *Optional:* Implement hierarchical clustering enhancement (e.g., `AgglomerativeClustering`) (Ref: 2F.1.iii).
    - [ ] *Optional:* Implement state augmentation enhancement (include `mean_rate`, `var_rate` in state) (Ref: 2F.1.iii).
    - [ ] Integrate clustering execution into the main training loop (run every 1000 steps) (Ref: 2F.1.ii).
    - [ ] Implement buffering (`rate_buffer`) for aggregating rates for infrequent clustering (Ref: 2E.3.iv).
    - [ ] Test: Unit test k-means implementation and dynamic k selection logic (range, score calc, override).
    - [ ] Test: Unit test edge case handling (small k override, empty cluster avg reward).
    - [ ] Test: Unit test novel domain handling (bifurcation trigger, misattribution handling).
    - [ ] Test: Unit test TD state stability mechanisms (soft clustering calc, weighted update, stable k adjustment logic, V_state init).
    - [ ] Test: Unit test optional enhancements (hierarchical clustering, state augmentation) if implemented.
  - [ ] **Test: Unit test Step 3 functionalities.**

- [ ] **Step 4: Implement Enhanced SIE Autonomy**
  - [ ] **Validation Check**: SIE guides learning effectively using cluster-based states, cluster-specific rewards, prevents gaming, resolves conflicts, and incorporates ethical adjustments (Ref: 2C).
  - [ ] **Success Statement**: I will know I achieved success on this step when SIE ranks goals for 7M neurons in <1ms and demonstrates robust, aligned reward calculation.
  - [ ] **Actionable Substeps**:
    - [x] Code core SIE logic (`sie.py`) & integrate (`unified_neuron.py`) (Ref: 2C).
    - [ ] Implement core reward formula: `total_reward = TD_error + novelty - habituation + self_benefit` (Ref: 2C.2.ii).
    - [ ] Implement TD Learning specifics (Ref: 2C.3):
        - [ ] Calculate TD(0) error: `TD_error = r + γ * V(next_state) - V(current_state)`.
        - [ ] Set discount factor `γ = 0.9`.
        - [ ] Set learning rate `α = 0.1`.
        - [ ] Update value function `V_states` using soft cluster probs: `V_states[idx] += α * TD_error * cluster_probs[idx]` (Ref: 2F.5.iii, 2C.3.ii).
        - [ ] Initialize `V_states` tensor to zeros, size `k_min` (Ref: 5A.2.ii).
    - [ ] Implement Novelty calculation: Store `recent_inputs`, compute `cosine_similarity`, `novelty = 1 - max(similarity)` (Ref: 2C.4).
    - [ ] Implement Habituation calculation: Store `habituation_counter`, increment if `similarity > 0.9`, decay periodically (`*= 0.95`), `habituation = counter[match]` (Ref: 2C.5).
    - [ ] Implement refined Self-Benefit calculation (homeostasis-based) (Ref: 2C.6):
        - [ ] Calculate formula: `self_benefit = 1 - abs(var(rates[-1k:]) - target_var) / target_var`.
        - [ ] Define `target_var = 0.05 Hz^2`. Clamp result `[0, 1]`.
    - [ ] Implement SIE modulation of STDP (Ref: 2C.7):
        - [ ] Calculate sigmoid mapping: `mod_factor = 2 * torch.sigmoid(total_reward) - 1`.
        - [ ] Calculate effective learning rate: `eta_effective = eta * (1 + mod_factor)`.
        - [ ] Apply final weight update: `Δw_ij(T) = eta_effective * total_reward * e_ij(T)`.
    - [ ] Implement cluster-specific reward allocation: Calculate `cluster_contrib`, `cluster_reward[c] = mean(total_reward[members]) + ...` (Ref: 2C.2.iv).
    - [ ] Implement multi-level selection conflict mitigation: Hierarchical selection (override local STDP if `cluster_reward < 0.5`), global alignment pressure (`cluster_reward += 0.1 * (global_reward - cluster_reward)`) (Ref: 2C.2.iv).
    - [ ] *Optional:* Implement localized SIE signals (calculate `dopamine_reward`, `acetylcholine_reward`, combine weighted by `receptor_density`) (Ref: 2C.2.iv).
    - [ ] Implement credit/blame attribution logic: Adjust `cluster_rewards` based on `cluster_contrib` if `total_reward < 0`; check `cross_contrib`, adjust `cross_connectivity` if routing failure (Ref: 2C.2.vi).
    - [ ] Implement evolutionary analogues for richness/adaptability: Environmental adaptation (`input_diversity -> novelty`), competition (`dopamine_reward -= 0.1 * competition_score`), arms race (`counter_adapt`), replication (`replicate_pathway`) (Ref: 2C.2.vii).
    - [ ] Implement SIE gaming prevention mechanisms (Ref: 2C.8.ii):
        - [ ] Implement robust reward formulation (weight external `r` higher, add `alignment_penalty`).
        - [ ] Implement co-evolutionary reward adjustment (`co_evolve_reward[c] = ...`).
        - [ ] Implement enhanced safeguards: Isolation Forest detector, `hacking_score` validation, dynamic capping, V-state regularization (`TD - λ * V`), drift monitoring (`drift_score`), stability constraints (variance check -> reduce `eta`), diversity monitoring (`output_diversity`), energy penalty, adversarial gaming tests, reward shaping (`alignment_bonus`).
        - [ ] Implement reward component normalization (`*_norm = (* - min) / (max - min)`).
        - [ ] Implement conflict management (`impact_adjusted = impact * (1 - novelty)`).
        - [ ] Implement oscillation prevention (damping factor `α = 1 - tanh(...)`, ε-greedy).
        - [ ] Implement dynamic SIE weight adjustment based on performance.
    - [ ] Implement Dynamic Ethics Adjuster mechanism (Ref: 2C.9, 9.D):
        - [ ] Implement violation detection logic based on encoded constraints.
        - [ ] Calculate dynamic penalty: `ethical_penalty = - severity * context_factor`.
        - [ ] Add penalty to `total_reward`.
    - [ ] Integrate `neuron_kernel.hip` for goal ranking (if applicable).
    - [x] Test: Unit test core SIE calculation & integration (`test_autonomy.py`).
    - [ ] Test: Unit test TD learning implementation (error calc, V_states update logic).
    - [ ] Test: Unit test Novelty calculation logic.
    - [ ] Test: Unit test Habituation calculation logic.
    - [ ] Test: Unit test Self-Benefit calculation logic.
    - [ ] Test: Unit test SIE modulation of STDP (mapping, effective rate, application).
    - [ ] Test: Unit test cluster reward allocation and conflict handling logic.
    - [ ] Test: Unit test optional localized SIE signals if implemented.
    - [ ] Test: Unit test credit/blame attribution logic.
    - [ ] Test: Unit test evolutionary analogues if implemented.
    - [ ] Test: Unit test gaming prevention mechanisms (robust reward formula, detector logic, individual safeguards).
    - [ ] Test: Unit test Dynamic Ethics Adjuster logic (detection, penalty calc, integration).
    - [ ] Test: Validate SIE correctness metric (`reward_correctness < 0.1`) (Ref: 2C.8.iii).
    - [ ] Test: Validate refined causal inference for credit assignment (accuracy target 95%) (Ref: 2C.8.iii).
    - [ ] Scale to 7M—verify <1ms SIE calculation time via logs.
  - [ ] **Test: Unit test Step 4 functionalities.**

- [ ] **Step 5: Implement Knowledge Graph & Structural Plasticity**
  - [ ] **Validation Check**: Implicit graph emerges, persists critical pathways, avoids pathological structures, and adapts structure based on performance stably and efficiently (Ref: 2D, 4C).
  - [ ] **Success Statement**: I will know I achieved success on this step when 7M neuron connections reflect domain relationships stably, efficiently, and adaptively, guided by plasticity rules.
  - [ ] **Actionable Substeps**:
    - [x] Code graph logic in STDP (`resonance_enhanced_stdp.py`) for hints (Ref: 2D).
    - [ ] Implement core Structural Plasticity algorithms (Growth, Pruning, Rewiring) triggered by SIE/activity metrics (`structural_plasticity.py`?) (Ref: 4C).
    - [ ] Implement enhanced plasticity triggers (Ref: 4.C.2.iv):
        - [ ] Calculate burst score: `burst_score = sum(rates[-5:] > 5*target)`. Trigger growth (`growth_rate *= 1.1`) if `burst_score > 0`.
        - [ ] Calculate BDNF proxy: `bdnf_proxy = rate/target`. Trigger growth (`growth_rate *= 1.1`) if `bdnf_proxy > 1.5`.
        - [ ] Implement critical period logic: `if timestep < 1M: growth_rate *= 2, rewire_rate *= 2`.
    - [ ] Implement pathway persistence/protection mechanism (Ref: 2D.3.iii, 4.C.3.i, 5.E.4):
        - [ ] Implement multi-criteria tagging logic (check standard, sparse, infrequent criteria). Store tags (e.g., sparse bool tensor `persistent`).
        - [ ] Implement dynamic persistence threshold adjustment: Decrease `w_threshold`, `reward_threshold` if `environmental_drift > 0.1`.
        - [ ] Implement enhanced de-tagging logic: Remove tag if low reward OR negative reward OR low diversity (`output_diversity[c] < 0.5`).
        - [ ] Modify decay and rewiring logic to exempt tagged synapses.
    - [ ] Implement pathology detection/pruning (Ref: 2D.5):
        - [ ] Calculate pathology score: `score = mean(rates[path] * (1 - diversity[path]))`.
        - [ ] Calculate graph entropy: `entropy = -sum(p*log(p))` from degree distribution `p`.
        - [ ] Implement proactive pruning trigger: Prune path if `score > 0.1` OR `entropy < 1`.
        - [ ] Implement delayed pruning option: Require trigger condition to persist (e.g., 100k/200k steps) before pruning.
    - [ ] Implement stability checks during/after plasticity events (Ref: 4.C.3):
        - [ ] Implement enhanced dynamic capping: Calculate `max_change = 0.01 * (1 - mean(rates)/0.5)`. Limit structural changes.
        - [ ] Implement proactive reversion: Calculate `interference_score`. Revert proposed changes if score is high.
        - [ ] Implement post-change reversion: Monitor `output_variance`. Revert changes if `variance_after > variance_before * 1.1` and `> 0.05 Hz`.
        - [ ] Implement structural integrity checks: Global neuron cap, criticality-driven rate adjustment, cluster connectivity monitoring, access preservation logic (Ref: 5E.4.iv).
        - [ ] Implement conflict resolution logic for persistent pathways (detect conflict, gradual STDP depression, SIE response, de-tagging trigger, structural adjustment) (Ref: 5E.4.iv).
    - [ ] Implement KG specialization via inhibition: Ensure inhibitory feedback suppresses irrelevant clusters (Ref: 2D.3.ii).
    - [ ] Implement KG cluster reinforcement via SIE: Strengthen intra-cluster connections if `cluster_reward[c] > 0.9` (Ref: 2D.3.ii).
    - [ ] Implement KG hierarchical organization validation: Analyze graph structure for hierarchical properties (Ref: 2D.3.iii).
    - [ ] Implement KG interference prevention mechanisms: Localize STDP, inhibitory suppression, routing protection (exempt persistent), routing specificity check (`cross_connectivity < 0.1` -> add connections), spike-timing homeostasis (Ref: 2D.4.vi).
    - [ ] Implement KG compositionality mechanisms: Cross-cluster STDP logic, SIE/TD sequencing reinforcement, temporal sequencing validation, cross-cluster validation/correction (`weaken_pathway`), inhibitory isolation, pathway protection (Ref: 2D.4.vii).
    - [ ] Implement KG efficiency optimization: Monitor `efficiency_score`, increase global inhibition if `< 0.3` (Ref: 2D.5.i).
    - [ ] Implement exploration mechanisms (Ref: 2B.8.iii):
        - [ ] Implement pathway recombination logic (select high-reward clusters, combine weight matrices).
        - [ ] Implement exaptation logic (`coopt_pathway(c, new_domain)`).
    - [ ] Integrate structural plasticity calls into the main simulation loop/`UnifiedNeuronModel`.
    - [ ] Implement sparse weight representation (e.g., CSR) and adapt plasticity/STDP logic (Ref: 5.A.2, 5.D.1).
    - [ ] Implement distributed lock mechanism for structural modifications (Ref: 5D.2.vi).
    - [ ] Implement structural plasticity cost management (async execution, buffering, prioritization, clustering optimization via sampling) (Ref: 5D.6.iii).
    - [ ] Implement KG ethical tracking/constraints (track pathways, constrain plasticity) (Ref: 9.D).
    - [x] Test: Unit test hint mechanism (`test_knowledge_graph.py`).
    - [ ] Test: Unit test core Growth, Pruning, Rewiring algorithms.
    - [ ] Test: Unit test enhanced plasticity triggers (burst calc, BDNF calc, critical period logic).
    - [ ] Test: Unit test persistence mechanisms (multi-criteria tagging logic, dynamic threshold logic, de-tagging logic, exemption logic).
    - [ ] Test: Unit test pathology detection/pruning (score calc, entropy calc, trigger logic, delayed pruning).
    - [ ] Test: Unit test stability checks for plasticity (dynamic capping calc, proactive reversion logic, post-change reversion logic, integrity checks).
    - [ ] Test: Unit test KG mechanisms (specialization inhibition, reinforcement logic, hierarchy validation, interference prevention logic, compositionality logic, efficiency optimization logic).
    - [ ] Test: Unit test exploration mechanisms (recombination logic, exaptation logic).
    - [ ] Test: Unit test sparse weight implementation.
    - [ ] Test: Unit test distributed lock for plasticity.
    - [ ] Test: Unit test plasticity cost management (async, buffering, priority, sampling).
    - [ ] Test: Unit test ethical constraints on plasticity.
    - [ ] Scale to 7M—verify memory usage and stability during plasticity.
  - [ ] **Test: Unit test Step 5 functionalities.**

- [ ] **Test: Integration test Task 1 components (LIF, STDP, SIE, Clustering, KG/Plasticity).**
- [ ] **Test: Unit tests for Task 1.**

### Task 2: Integrate Multimodal Inputs
- [ ] **Success Statement**: I will know I achieved success on this task when enhanced encoders/decoders support autonomous interpretation and generation across modalities, validated through testing.
- **Current Status**: *Partial*—Basic encoders/decoder implemented and tested. Advanced features, integration, and streaming pending.

- [ ] **Step 1: Implement Enhanced Multimodal Encoders**
  - [ ] **Validation Check**: Encoders process text, image, video, audio using hierarchical and temporal schemes effectively (Ref: 3A.2).
  - [ ] **Success Statement**: I will know I achieved success on this step when all modalities stream information-rich spikes continuously.
  - [ ] **Actionable Substeps**:
    - [x] Code basic `encoder.py` sub-encoders (Ref: 3A).
    - [ ] Implement Poisson spike generation with 5ms refractory period for input neurons (Ref: 3A.3.ii).
    - [ ] Implement hierarchical encoding enhancements (e.g., text: char/word/sentence layers; image: pixel/edge/object layers) (Ref: 3A.2.ii).
    - [ ] Implement spike pattern encoding enhancements (encode features via precise timing within 50ms window) (Ref: 3A.2.iii).
    - [ ] Implement encoding robustness: Apply low-pass filter to input frequencies (Ref: 5E.5.ii).
    - [x] Test: Unit test basic encoders (`test_io.py`).
    - [ ] Test: Unit test input neuron refractory period implementation.
    - [ ] Test: Unit test enhanced encoding schemes (hierarchical layers, pattern generation).
    - [ ] Test: Unit test encoding robustness filter application.
    - [ ] Integrate encoders with `run_phase3.py` for continuous/streaming operation.
    - [ ] Test: Validate handling of multi-domain inputs (temporal separation, sequential cluster activation, inhibitory suppression) (Ref: 2D.4.iv).
  - [ ] **Test: Unit test Step 1 functionalities.**

- [ ] **Step 2: Implement Flexible Decoder**
  - [ ] **Validation Check**: Decoder outputs varied formats (text, structured, silent) based on context/SIE state accurately (Ref: 3B).
  - [ ] **Success Statement**: I will know I achieved success on this step when FUM signals goals flexibly, appropriately, and accurately.
  - [ ] **Actionable Substeps**:
    - [x] Code basic rate decoder (`decode_text_rate`) in `decoder.py` (Ref: 3B.2).
    - [ ] Implement temporal decoding for structured output (map rates in sequential windows to tokens via lookup table) (Ref: 3B.2.ii).
    - [ ] Implement mode selection logic (choose text, voice, visual, silent based on SIE state / context) (Ref: Roadmap/3B).
    - [ ] Implement decoder execution on CPU, logging outputs to SSD (Ref: 3B.4.i).
    - [x] Test: Unit test basic text decoding (`test_io.py`).
    - [ ] Test: Unit test temporal decoding logic (windowing, mapping).
    - [ ] Test: Unit test mode selection logic.
    - [ ] Test: Unit test voice/visual output generation (placeholder/basic implementation).
    - [ ] Test: Unit test CPU execution and logging mechanism.
  - [ ] **Test: Unit test Step 2 functionalities.**

- [ ] **Test: Integration test Task 2 components (Encoders, Decoders).**
- [ ] **Test: Unit tests for Task 2.**

### Task 3: Optimize for Workstation & Prepare for Scaling
- [ ] **Success Statement**: I will know I achieved success on this task when FUM fits 56GB VRAM with constant learning at <5% GPU usage, leveraging kernels, efficient memory management, and robust distributed logic, validated through testing.
- **Current Status**: *Incomplete*—Kernels, sharding, advanced memory/distributed logic not coded.

- [ ] **Step 1: Code & Integrate Compute Kernels**
  - [ ] **Validation Check**: Kernels run LIF, STDP correctly and meet performance targets (<5s inference) (Ref: 2E, 5.D.5).
  - [ ] **Success Statement**: I will know I achieved success on this step when kernels process 7M neurons efficiently and correctly.
  - [ ] **Actionable Substeps**:
    - [ ] Review kernel code (`neuron_kernel.hip`, `neural_sheath.cpp`) (Ref: 2A.7, 2B.6, 2E).
    - [ ] Implement Python wrappers/bindings using `ctypes` or `torch.utils.cpp_extension` (Ref: 5.D.5, 5D.5.ii).
    - [ ] Integrate wrappers into `UnifiedNeuronModel` / STDP handler with backend switching logic (Ref: 5.D.5).
    - [ ] Ensure kernel integration respects hardware roles (LIF kernel on 7900XTX) (Ref: 2E.2).
    - [ ] Ensure kernel uses correct data types (`float16` compute, `uint8` history) (Ref: 2A.7).
    - [ ] Verify kernel responsibilities (LIF kernel excludes STDP/traces) (Ref: 2A.7.ii).
    - [ ] Compile kernels using `hipcc` (Requires HIP SDK setup or CUDA equivalent path).
    - [ ] Use profiling tools (`rocprof`) to identify kernel bottlenecks (Ref: 5D.5.ii).
    - [ ] Test: Unit test kernel wrappers and data marshalling logic.
    - [ ] Test: Integration test comparing kernel output vs. PyTorch output for functional equivalence.
    - [ ] Test: Benchmark kernel performance at 1000 neurons (target <5s inference, <5% GPU usage).
    - [ ] Scale to 7M—verify performance and VRAM usage.
  - [ ] **Test: Unit test Step 1 functionalities.**

- [ ] **Step 2: Implement Distributed Computation & Memory Management**
  - [ ] **Validation Check**: Sharding fits 7M neurons in 56GB VRAM, async updates are stable, caching is effective, fault tolerance works (Ref: 5.D).
  - [ ] **Success Statement**: I will know I achieved success on this step when FUM shards dynamically without overflow, communicates efficiently, and manages memory effectively.
  - [ ] **Actionable Substeps**:
    - [ ] Implement sparse weight representation using CSR format (`torch.sparse_csr_tensor`) (Ref: 5.A.2, 5.D.1).
        - [ ] Implement compression ratio validation (~9:1 target) and mitigation (block CSR, adjust sparsity) (Ref: 5A.2.i).
    - [ ] Implement graph partitioning using METIS wrapper (Ref: 5.D.1).
        - [ ] Validate partitioning effectiveness (>95% target) and test on heterogeneous hardware (Ref: 5D.1.iii).
    - [ ] Implement inter-GPU/node communication layer using `torch.distributed` non-blocking ops, MPI/RCCL, or RDMA for spikes (Ref: 5.D.1, 5D.2.x).
    - [ ] Implement asynchronous update logic (Ref: 5.D.2):
        - [ ] Set skew tolerance target to 1ms (Ref: 5D.2.iii).
        - [ ] Implement high-precision synchronization using PTP (Ref: 5D.2.iii).
        - [ ] Implement jitter mitigation via temporal integration (avg timings over ~5ms) (Ref: 5D.2.iii, 5D.2.vii).
        - [ ] Prioritize local clocks for STDP calculations (Ref: 5D.2.iii).
        - [ ] Implement state divergence correction (broadcast global state after sync) (Ref: 5D.2.iv).
        - [ ] Implement vector clocks for conflict-free weight and trace updates (Ref: 5D.2.v).
        - [ ] Implement latency mitigation suite (timestamp correction, adaptive STDP window, latency-aware scaling, Kalman filter, cross-shard validation) (Ref: 5D.2.vii).
        - [ ] Implement resource contention handling suite (robust buffering with context preservation, priority scheduling, debt monitoring/mitigation, stability checks) (Ref: 5D.2.viii).
        - [ ] Implement stochastic modeling (Markov chains) and adaptive thresholds for overruns (Ref: 5E.5.viii).
    - [ ] Implement parameter server logic / caching strategy (`memory_manager.py`?) (Ref: 5.D.4):
        - [ ] Implement LRU + Priority Queuing logic (`priority = abs(w) * co_act`).
        - [ ] Implement pre-fetching logic (predict likely spiking neurons `mean(history) > 0.1 Hz`).
        - [ ] Set target cache size (~10% VRAM).
        - [ ] Implement `CacheManager` / `PriorityLRUCache` classes.
    - [ ] Implement fault tolerance mechanisms (Ref: 5.D.3, 5.D.4):
        - [ ] Utilize ECC memory where available (Ref: 5D.4.ii).
        - [ ] Implement redundancy/checkpointing/restoration for non-ECC hardware/errors (Ref: 5D.4.ii).
        - [ ] Implement Raft-based node failure handling (task reassignment) (Ref: 5D.3.i).
        - [ ] Implement network partition handling (isolated mode operation, state reconciliation) (Ref: 5D.3.ii).
        - [ ] Implement bottleneck handling (load monitoring, task offloading) (Ref: 5D.3.iii).
    - [ ] Implement distributed lock mechanism for structural modifications (Ref: 5D.2.vi).
    - [ ] Implement hardware agnosticism strategies (FLOPS normalization, adaptive allocation, network mitigations) (Ref: 5D.5.iv).
    - [ ] Implement resource efficiency optimizations (sparse data structures, efficient algorithms, dynamic allocation) (Ref: 6.C.2.i).
    - [ ] Implement practical engineering strategies (Docker containerization, distributed tracing, automated maintenance) (Ref: 5D.7).
    - [ ] Implement complexity management strategies (unified framework, incremental deployment, simulation, decentralization, fault tolerance, graceful degradation, anomaly detection) (Ref: 5D.8).
    - [ ] Test: Unit test sparse matrix operations and compression validation logic.
    - [ ] Test: Unit test partitioning logic and effectiveness validation.
    - [ ] Test: Unit test communication layer implementation (incl. RDMA if used).
    - [ ] Test: Unit test async update synchronization (PTP, skew target), vector clocks, latency mitigation suite.
    - [ ] Test: Unit test resource contention handling suite (buffering, scheduling, debt logic).
    - [ ] Test: Unit test caching effectiveness (hit rate, priority logic, pre-fetch logic).
    - [ ] Test: Unit test fault tolerance mechanisms (Raft integration, partition logic, bottleneck logic, memory recovery).
    - [ ] Test: Unit test distributed lock mechanism implementation.
    - [ ] Test: Unit test hardware agnosticism adaptations (scaling factor, load reduction).
    - [ ] Test: Unit test engineering strategies (deployment scripts, tracing logs, maintenance automation).
    - [ ] Test: Validate skew impact remains bounded (<0.01 target) (Ref: 5D.2.ix).
    - [ ] Test: Validate communication/sync overhead remains low (<0.005s target) (Ref: 5D.2.x).
    - [ ] Test: Validate thermal monitoring and mitigation logic (Ref: 6.A.1.ii).
    - [ ] Test: Validate sync overhead remains low (<7.5% cycle target) (Ref: 6.A.1.iii).
    - [ ] Scale to 7M—verify VRAM usage, partitioning effectiveness, communication overhead, stability.
  - [ ] **Test: Unit test Step 2 functionalities.**

- [ ] **Test: Integration test Task 3 components (Kernels, Sharding, Memory, Communication, Fault Tolerance).**
- [ ] **Test: Unit tests for Task 3.**

### Task 4: Train, Validate, and Debug
- [ ] **Success Statement**: I will know I achieved success on this task when FUM autonomously achieves superintelligence (>95% accuracy on complex benchmarks) with 300 inputs, demonstrates stable continuous learning, and debugging tools are effective, validated through comprehensive testing.
- **Current Status**: *Incomplete*.

- [ ] **Step 1: Implement Training Orchestration & Seed Training (Phase 1)**
  - [ ] **Validation Check**: Training loop runs correctly, integrating all components. 80 curated inputs bootstrap basic learning and primitive formation (Ref: 5A).
  - [ ] **Success Statement**: I will know I achieved success on this step when FUM processes 80 inputs with >50% accuracy in ~2-3 hours, showing evidence of primitive formation guided by initial priors.
  - [ ] **Actionable Substeps**:
    - [ ] Implement initial data curation/validation logic for seed corpus (80 inputs) (Ref: 5.A.2):
        - [ ] Calculate semantic coverage using embeddings, target >0.9 (Ref: 5A.2.i).
        - [ ] Select inputs including 2-5 edge cases per domain (Ref: 5A.2.i).
        - [ ] Calculate concept diversity using embeddings, target >0.7 (Ref: 5A.2.i).
        - [ ] Calculate input complexity using domain scores, target >3 (Ref: 5A.2.i).
        - [ ] Assess bias (cultural, stylistic, demographic) using dictionaries/Fairness Indicators, target <0.5 (Ref: 5A.2.i).
        - [ ] Mitigate bias via resampling or constrained selection (Ref: 5A.2.i).
        - [ ] Calculate primitive coverage using embeddings, target >0.9 (Ref: 5A.2.i).
        - [ ] Validate STDP/SIE triggering for selected inputs (Ref: 5A.2.i).
        - [ ] Detect concept gaps (`1 - primitive_coverage > 0.1`) and augment input set (Ref: 5A.2.i).
        - [ ] Construct validation set (16 inputs) using stratified sampling based on domain concepts (Ref: 5A.2.i).
    - [ ] Implement initial network setup including connectivity priors (distance bias `exp(-d/σ)`, `σ=10`, use `torch.multinomial`) (Ref: 4.E.3, 5.A.2).
        - [ ] Initialize weights using specified distributions (`U(0, 0.3)` E, `U(-0.3, 0)` I) (Ref: 5A.2.i).
        - [ ] Initialize eligibility traces (`e_ij`) and value function (`V_states`) to zero (Ref: 5A.2.ii).
    - [ ] Implement Phase 1 training script (`run_phase1.py`) integrating Task 1, 2, & 3 components (Ref: 5A.2).
        - [ ] Implement loop iterating through shuffled corpus for 5-10 epochs (Ref: 5A.2.iv).
        - [ ] Implement minimal SIE guidance (focus on novelty/habituation components) (Ref: 5A.2.iv).
    - [ ] Implement basic validation logic within training script or `benchmark.py` (Ref: 5.A.2, 6.A.6).
        - [ ] Calculate accuracy on validation set (target >50%).
        - [ ] Implement checks for primitive formation reliability.
        - [ ] Implement check for basic goal generation rate.
    - [ ] Run training with 80 curated seed inputs.
    - [ ] Validate Phase 1 outcomes against targets: accuracy (>50%), primitive formation, goal generation/hour, variance (< 0.1 Hz²), sparsity (>95%), avg weight (|w| ≈ 0.01-0.05) (Ref: 5A.4.i).
    - [ ] Validate Phase 1 metrics against targets: criticality index < 0.1, variance < 0.05 Hz, 90% retention over 1M steps (Ref: 6.A.7.i).
  - [ ] **Test: Unit test Step 1 functionalities (data curation logic, script execution, validation calculations, outcome metric checks).**

- [ ] **Step 2: Scale Training & Validate Generalization (Phase 2)**
  - [ ] **Validation Check**: 300 inputs deepen learning, performance improves significantly, and model demonstrates true generalization on diverse tests (Ref: 5B, 1.A.3, 5.E.8, 6.A.7).
  - [ ] **Success Statement**: I will know I achieved success on this step when FUM hits >85-95% accuracy on target benchmarks and generalization tests.
  - [ ] **Actionable Substeps**:
    - [ ] Implement Phase 2 training script (`run_phase2.py`) using data curriculum (up to 300 total inputs) (Ref: 5B).
    - [ ] Implement data loading logic for scaling batches.
    - [ ] Implement Phase 2 loop enhancements (activate targeted SIE, KG monitoring, cluster perf tracking, initiate structural plasticity growth trigger) (Ref: 5B.2.ii).
    - [ ] Implement enhanced generalization validation framework (Ref: 1.A.3, 5.E.8, 6.A.7):
        - [ ] Implement emergent synthetic data generation using KG (`generate_emergent_inputs`).
        - [ ] Implement curated real-world sample testing logic (~1000 examples/domain, target >80% accuracy) (Ref: 01_Introduction.md).
        - [ ] Implement SNN-specific adversarial input generation (`generate_snn_adversarial`) and testing (target >80% accuracy) (Ref: 1.A.3.i).
        - [ ] Implement OOD analysis: Calculate KL divergence `shift_score`, target >0.5 (Ref: 1.A.3.i).
        - [ ] Implement memorization detection: Calculate accuracy difference `memorization_score`, target < 0.1 (Ref: 1.A.3.i).
        - [ ] Implement brittleness testing: Generate SIE-guided perturbations (`perturb_inputs`), target `perturbed_accuracy > 0.8` (Ref: 1.A.3.i).
        - [ ] Implement rare regime testing: Generate rare inputs (`generate_rare_inputs`), target `rare_accuracy > 0.8` (Ref: 1.A.4.i).
        - [ ] Implement emergent failure mode detection: Train GAN on activity history (`EmergentFailureDetector`), test synthesized modes, target `failure_score < 0.1` (Ref: 1.A.4.i).
        - [ ] Implement state space sampling (stratified) and dynamic validation updates based on samples (Ref: 1.A.4.i).
        - [ ] Implement junk data injection testing logic (target >84% accuracy) (Ref: 6.A.7.i).
        - [ ] Implement primitive generalization test logic (Ref: 5E.8.ii).
        - [ ] Implement emergent graph generalization test logic (target >0.9 routing accuracy) (Ref: 5E.8.ii).
        - [ ] Implement comparative benchmarking logic against transformer baseline (Ref: 1.B.3.ii).
    - [ ] Run training with up to 300 inputs.
    - [ ] Validate Phase 2 outcomes against targets: accuracy (>85-95% on specific benchmark subsets: MATH Alg 1-5, GPQA 1-3, HE subset, CNN/DM subset, Custom Physics) (Ref: 1.A.7.i), inference time (<5s), generalization metrics, refined graph (`w ≈ 0.8`), inter-domain links (Ref: 5B.4.i).
    - [ ] Validate Phase 2 metrics against targets: 95% retention, 90% cross-domain consistency (Ref: 6.A.7.i).
  - [ ] **Test: Unit test Step 2 functionalities (script logic, data loading, validation framework components and metrics).**

- [ ] **Step 3: Implement & Validate Continuous Operation & Stability (Phase 3)**
  - [ ] **Validation Check**: FUM runs stably 24/7 with live/streaming data, demonstrating autonomy, robust SOC, effective debugging, and long-term alignment (Ref: 5C, 5.E.4, 5.E.11).
  - [ ] **Success Statement**: I will know I achieved success on this step when FUM processes real-world inputs autonomously, maintains stability, and issues can be effectively diagnosed.
  - [ ] **Actionable Substeps**:
    - [ ] Implement Phase 3 continuous learning script (`run_phase3.py`) (Ref: 5C).
        - [ ] Implement handling for continuous streams of potentially unlabeled data (Ref: 5C.2.i).
        - [ ] Implement logic for SIE evaluation relying primarily on internal metrics when external `r` is absent (Ref: 5C.2.ii).
        - [ ] Implement periodic saving/loading of full network state (V, w, e_ij, V_states, etc.) to SSD using efficient serialization (Ref: 5C.2.ii).
        - [ ] Activate full structural plasticity mechanisms (Growth, Pruning, Rewiring) based on monitored metrics (Ref: 5C.2.ii).
    - [ ] Implement SOC management mechanisms (Ref: 5C.3, 4.A.3):
        - [ ] Calculate dynamic thresholds based on recent variance.
        - [ ] Implement adaptive tuning of global inhibition based on sustained variance.
        - [ ] Implement avalanche detection logic (monitor spike sums).
        - [ ] Implement inhibitory response logic (increase global inhibition).
        - [ ] Implement variance regulation logic (reduce `eta` if variance high).
        - [ ] Implement structural adjustment logic (prune high-rate neurons during avalanches).
        - [ ] Implement enhanced early warning system (calculate `early_warning` metric, trigger preemptive inhibition).
        - [ ] Implement criticality monitoring (calculate `criticality_index`).
        - [ ] Implement dynamic intervention logic (adjust inhibition/plasticity based on index).
        - [ ] Implement predictive criticality controller (`CriticalityController` using NN).
        - [ ] Implement adaptive criticality tuning logic (adjust plasticity based on index).
        - [ ] Implement connection density control logic.
        - [ ] Implement E/I ratio stability checks.
        - [ ] Implement interaction analysis (correlation matrix) and damping logic.
        - [ ] Implement decentralized control logic assignment.
    - [ ] Implement enhanced debugging/interpretability tools (`UnifiedDebugger` framework) (Ref: 5.E.2, 5.E.11):
        - [ ] Implement Spike Pathway Tracing component (log timestamps, reconstruct paths) (Ref: 5E.2.v, 5E.11.ii).
        - [ ] Implement Reasoning Audit Tool (analyze pathways for inconsistencies) (Ref: 5E.2.v).
        - [ ] Implement Causal Inference Engine component (use Bayesian nets) (Ref: 5E.11.ii).
        - [ ] Implement Predictive Debugging Model component (use RL) (Ref: 5E.11.ii).
        - [ ] Implement scalable interpretability methods (sampled tracing via BFS, hierarchical cluster analysis, causal pathway analysis via interventions, generative model interpretation, synaptic contribution analysis) (Ref: 5E.2.vi).
        - [ ] Implement scalable control/debug/tuning methods (hierarchical sampling, distributed logging via Cassandra, hierarchical tuning, spike correlation debugging, graph analysis insights, sampled model checking via NuSMV, offloading) (Ref: 5E.2.vii).
    - [ ] Implement long-term stability/drift monitoring and correction (Ref: 5.E.4):
        - [ ] Implement synaptic decay with selective modulation (slower for low activity, faster for low reward/conflict) (Ref: 5E.4.iv).
        - [ ] Implement enhanced de-tagging criteria (check low output diversity) (Ref: 5E.4.iii).
        - [ ] Implement dynamic persistence threshold adjustment based on drift (Ref: 5E.4.iii).
        - [ ] Implement protection for infrequent knowledge (extended window, activity-independent tagging, lower thresholds) (Ref: 5E.4.iii).
        - [ ] Implement model calibration/drift monitoring (calculate `calibration_error`, `drift_score`, target <0.1) and correction (reset SIE weights, increase ground truth freq) (Ref: 5E.4.iii).
        - [ ] Implement continual learning balance checks (monitor validation accuracy, target >95% retention) (Ref: 5E.4.iv).
        - [ ] Implement structural integrity checks (neuron cap, criticality adjustment, cluster connectivity, access preservation) (Ref: 5E.4.iv).
        - [ ] Implement conflict resolution logic for persistent pathways (detect conflict, gradual depression, SIE response, de-tagging, structural adjustment) (Ref: 5E.4.iv).
        - [ ] Implement non-functional complexification monitoring (complexity vs performance) and mitigation (reduce plasticity rates) (Ref: 4.C.3.iii).
        - [ ] Implement long-term adaptiveness monitoring (calculate `adaptiveness_score`) and reversion logic (Ref: 2B.9.ii).
    - [ ] Implement Phase 3 pathway redundancy logic (duplicate critical pathways) (Ref: 4.C.3.i).
    - [ ] Implement mechanisms for open-endedness (Ref: 4.F.2.ii):
        - [ ] Implement contingent adaptation (random rewire if `avg_reward[c] < 0.5` persists).
        - [ ] Implement diversity pressure (calculate `diversity_pressure`, modulate `novelty`).
    - [ ] Implement robustness to input noise/anomalies (Ref: 5.E.5):
        - [ ] Implement SNN dynamics robustness checks (monitor excessive rates `>1 Hz avg`).
        - [ ] Implement SIE robustness measures (smooth reward over inputs, cap reward if `novelty > 0.9`).
    - [ ] Implement emergence uncertainty management (Ref: 5E.7.xii):
        - [ ] Implement emergent behavior modeling (GANs on spike history).
        - [ ] Implement runtime anomaly detection (Isolation Forest on metrics).
        - [ ] Implement behavioral alignment monitoring (`task_alignment > 0.9`).
        - [ ] Implement reward shaping (add `gaming_penalty` based on diversity).
    - [ ] Deploy `run_phase3.py` with sample streaming data.
    - [ ] Test autonomy (measure goal generation rate, novelty metrics).
    - [ ] Test stability (measure variance, SOC metrics, response to perturbations, long-term drift score).
    - [ ] Test debugging framework effectiveness on injected faults (target 99% identification accuracy) (Ref: 5E.11.iii).
    - [ ] Verify resource usage (target <5% GPU idle) and persistence mechanism (save/load).
    - [ ] Validate Phase 3 metrics against targets: 95% retention/consistency, <1% control overhead (Ref: 6.A.7.i).
    - [ ] Validate primitive failure detection/mitigation logic (inject failures, check response) (Ref: 6.A.6.i).
    - [ ] Validate continuous validation monitoring and re-validation trigger logic (Ref: 6.A.9.i).
    - [ ] Validate control system correctness/stability (modular interactions, runtime tests, anomaly detection, fallback logic) (Ref: 5E.7.x, 5E.7.xi).
    - [ ] *Optional:* Implement investigation of consciousness markers (measure IIT, recurrent activity) (Ref: 9.A.1, 9.E).
  - [ ] **Test: Unit test Step 3 functionalities (script logic, persistence, SOC mechanisms, debugging tools, stability mechanisms).**

- [ ] **Step 4: Implement Supporting Models & Analysis**
  - [ ] **Validation Check**: Supporting models accurately predict scaling dynamics and phase transitions, formal methods are reliable, and risk analysis is complete.
  - [ ] **Success Statement**: I will know I achieved success on this step when supporting models are validated and integrated, providing reliable guidance for scaling and stability.
  - [ ] **Actionable Substeps**:
    - [ ] Implement Scaling Dynamics Model using dynamical systems analysis (coupled differential equations) (Ref: 2G.2).
        - [ ] Validate model predictive accuracy (>90% target) against intermediate scaling results (Ref: 2G.3).
    - [ ] Implement Phase Transition Predictor using bifurcation analysis on Scaling Dynamics Model (Ref: 2H.2).
        - [ ] Validate predictor accuracy (>95% target) against intermediate scaling results (Ref: 2H.3).
    - [ ] Implement Lyapunov stability analysis for STDP-SIE interaction (target 95% convergence rate) (Ref: 2B.7.v).
    - [ ] Implement formal methods reliability management (Ref: 5E.6.iii):
        - [ ] Implement calculation of brain-inspired validation metrics (Spike Lyapunov `V_spike`, SIE Alignment Score `alignment_score`).
        - [ ] Implement approximation-free methods (e.g., distributed reduction) where feasible.
        - [ ] Implement approximation error management (characterize bounds, track cumulative error, perform sensitivity analysis, implement fallback logic).
        - [ ] Implement false security prevention mechanisms (spike error detection via variance, dynamic alignment monitoring, assumption monitoring, stable state fallback).
    - [ ] Implement risk analysis models:
        - [ ] Implement Probabilistic Failure Model using Monte Carlo simulation (Ref: 6.E.2).
        - [ ] Implement Failure Impact Model using Fault Tree Analysis (FTA) (Ref: 6.F.2).
    - [ ] Implement interaction validation strategy (Ref: 2G.2):
        - [ ] Perform isolated enhancement testing (target 95% convergence).
        - [ ] Perform combined enhancement testing using control theory analysis (target 90% stability).
        - [ ] Implement continuous interaction monitoring (e.g., correlation metrics).
    - [ ] Implement scaling theory analysis: Track metrics (integrated info, graph complexity), develop scaling models (e.g., power laws), validate at 10M/1B neuron scales (Ref: 5D.10).
    - [ ] Test: Unit test Scaling Dynamics Model implementation and validation logic.
    - [ ] Test: Unit test Phase Transition Predictor implementation and validation logic.
    - [ ] Test: Unit test Lyapunov analysis implementation.
    - [ ] Test: Unit test formal methods reliability management components (metrics, error handling, prevention).
    - [ ] Test: Unit test risk analysis model implementations (Monte Carlo, FTA).
    - [ ] Test: Unit test interaction validation monitoring logic.
    - [ ] Test: Unit test scaling theory analysis implementation (metrics, model fitting).
  - [ ] **Test: Unit test Step 4 functionalities.**

- [ ] **Test: Integration test Task 4 components (Training Loop, Validation, Continuous Op, Debugging, Supporting Models).**
- [ ] **Test: Unit tests for Task 4.**

- [ ] **Test: System test for Phase 3 (End-to-end functionality, performance, stability).**
- [ ] **Test: Integration tests for Phase 3 (Cross-task component interactions).**
- [ ] **Test: Unit tests for Phase 3 (Consolidated unit test suite run).**
