# References

This list provides citations for the concepts, algorithms, and frameworks mentioned in the FUM document.

*   AMD. (n.d.). *ROCm Documentation*. Retrieved March 30, 2025, from https://rocm.docs.amd.com/
*   Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. *arXiv preprint arXiv:1606.06565*.
*   Arcuri, A., & Briand, L. C. (2011, March). A practical guide for using statistical tests to assess randomized algorithms in software engineering. In *2011 33rd International Conference on Software Engineering (ICSE)* (pp. 1-10). IEEE.
*   Åström, K. J., & Murray, R. M. (2008). *Feedback systems: An introduction for scientists and engineers*. Princeton University Press.
*   Bak, P., Tang, C., & Wiesenfeld, K. (1987). Self-organized criticality: An explanation of the 1/f noise. *Physical Review Letters*, *59*(4), 381-384.
*   Baldwin, C. Y., & Clark, K. B. (2000). *Design rules: The power of modularity* (Vol. 1). MIT press.
*   Barto, A. G., & Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning. *Discrete Event Dynamic Systems*, *13*(4), 341-379.
*   Beggs, J. M., & Plenz, D. (2003). Neuronal avalanches in neocortical circuits. *Journal of Neuroscience*, *23*(35), 11167-11177.
*   Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., ... & Zhang, Y. (2018). AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. *arXiv preprint arXiv:1810.01943*.
*   Berger, J. O. (1985). *Statistical decision theory and Bayesian analysis*. Springer Science & Business Media.
*   Bi, G. Q., & Poo, M. M. (1998). Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type. *Journal of Neuroscience*, *18*(24), 10464-10472.
*   Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.
*   Boyd, S., & Vandenberghe, L. (2004). *Convex optimization*. Cambridge University Press.
*   Brette, R. (2015). Philosophy of the spike: rate-based vs. spike-based theories of computation. *Frontiers in Systems Neuroscience*, *9*, 151. https://doi.org/10.3389/fnsys.2015.00151
*   Buluç, A., Fineman, J. T., Frigo, M., Gilbert, J. R., & Leiserson, C. E. (2009). Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks. In *Proceedings of the twenty-first annual symposium on Parallelism in algorithms and architectures* (pp. 233-244).
*   Burkitt, A. N. (2006). A review of the leaky integrate-and-fire neuron model. *Biological Cybernetics*, *95*(1), 1-19.
*   Buzsáki, G. (2006). *Rhythms of the brain*. Oxford University Press.
*   Buzsáki, G. (2010). Neural syntax: cell assemblies, synapsembles, and readers. *Neuron*, *68*(3), 362-385.
*   Camacho, E. F., & Bordons, C. (2007). *Model predictive control*. Springer Science & Business Media.
*   Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys (CSUR)*, *41*(3), 1-58.
*   Chklovskii, D. B., Mel, B. W., & Svoboda, K. (2004). Cortical rewiring and information storage. *Nature*, *431*(7010), 782-788.
*   Chung, F. R. (1997). *Spectral graph theory*. American Mathematical Society.
*   Cimatti, A., Clarke, E., Giunchiglia, E., Giunchiglia, F., Pistore, M., Roveri, M., ... & Tacchella, A. (2002). NuSMV 2: An opensource tool for symbolic model checking. In *Computer Aided Verification* (pp. 359-364). Springer Berlin Heidelberg.
*   Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). *Model checking*. MIT press.
*   Cochran, W. G. (1977). *Sampling techniques* (3rd ed.). Wiley.
*   Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). *Introduction to algorithms* (3rd ed.). MIT Press.
*   Cover, T. M., & Thomas, J. A. (2006). *Elements of information theory*. John Wiley & Sons.
*   Dean, J., & Ghemawat, S. (2008). MapReduce: simplified data processing on large clusters. *Communications of the ACM*, *51*(1), 107-113.
*   Deb, K. (2001). *Multi-objective optimization using evolutionary algorithms*. John Wiley & Sons.
*   Dehaene, S., & Changeux, J. P. (1997). A hierarchical neuronal network for planning behavior. *Proceedings of the National Academy of Sciences*, *94*(24), 13293-13298.
*   Destexhe, A., & Marder, E. (2004). Plasticity in single neuron and circuit computations. *Nature*, *431*(7010), 789-795.
*   Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)* (pp. 4171-4186).
*   Diestel, R. (2017). *Graph theory* (5th ed.). Springer.
*   Felleman, D. J., & Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. *Cerebral Cortex*, *1*(1), 1-47.
*   Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., ... & Jiang, D. (2020). CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In *Findings of the Association for Computational Linguistics: EMNLP 2020* (pp. 1536-1547).
*   Fidge, C. J. (1988). Timestamps in message-passing systems that preserve the partial ordering. In *Proceedings of the 11th Australian Computer Science Conference* (Vol. 10, No. 1, pp. 56-66).
*   Florian, R. V. (2007). Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity. *Neural Computation*, *19*(6), 1468-1502.
*   Foster, D. J., & Wilson, M. A. (2006). Reverse replay of behavioural sequences in hippocampal place cells during the awake state. *Nature*, *440*(7084), 680-683.
*   Frémaux, N., & Gerstner, W. (2016). Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules. *Frontiers in Neural Circuits*, *9*, 85. https://doi.org/10.3389/fncir.2015.00085
*   Gerstner, W., & Kistler, W. M. (2002). *Spiking neuron models: Single neurons, populations, plasticity*. Cambridge University Press.
*   Gerstner, W., Kistler, W. M., Naud, R., & Paninski, L. (2014). *Neuronal dynamics: From single neurons to networks and models of cognition*. Cambridge University Press.
*   Gilbert, S., & Lynch, N. (2002). Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services. *ACM SIGACT News*, *33*(2), 51-59.
*   Goebel, R., Sanfelice, R. G., & Teel, A. R. (2012). *Hybrid dynamical systems: modeling, stability, and robustness*. Princeton University Press.
*   Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative adversarial nets. In *Advances in Neural Information Processing Systems 27*.
*   Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). Explaining and harnessing adversarial examples. In *International Conference on Learning Representations (ICLR)*.
*   Häusser, M., & Mel, B. (2003). Dendrites: bug or feature?. *Current Opinion in Neurobiology*, *13*(3), 372-383.
*   Helias, M., Tetzlaff, T., & Diesmann, M. (2014). The correlation structure of local neuronal networks intrinsically results from recurrent connectivity. *PLoS Computational Biology*, *10*(1), e1003458. https://doi.org/10.1371/journal.pcbi.1003458
*   Hendrycks, D., & Dietterich, T. (2019). Benchmarking neural network robustness to common corruptions and perturbations. In *International Conference on Learning Representations (ICLR)*.
*   Hensch, T. K. (2004). Critical period regulation. *Annual Review of Neuroscience*, *27*, 549-579.
*   Hodgkin, A. L., & Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. *The Journal of Physiology*, *117*(4), 500-544.
*   Hogan, A., Blomqvist, E., Cochez, M., d'Amato, C., Melo, G. D., Gutierrez, C., Kirrane, S., Gayo, J. E. L., Navigli, R., Neumaier, S., Ngomo, A. C. N., Polleres, A., Rashid, S. M., Rula, A., Schmelzeisen, L., Sequeda, J., Staab, S., & Zimmermann, A. (2021). Knowledge graphs. *ACM Computing Surveys (CSUR)*, *54*(4), 1-37. https://doi.org/10.1145/3447772
*   Holtmaat, A., & Svoboda, K. (2009). Experience-dependent structural synaptic plasticity in the mammalian brain. *Nature Reviews Neuroscience*, *10*(9), 647-658.
*   Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. *Proceedings of the National Academy of Sciences*, *79*(8), 2554-2558.
*   Izhikevich, E. M. (2003). Simple model of spiking neurons. *IEEE Transactions on Neural Networks*, *14*(6), 1569-1572.
*   Izhikevich, E. M. (2007). Solving the distal reward problem through linkage of STDP and dopamine signaling. *Cerebral Cortex*, *17*(10), 2443-2452. https://doi.org/10.1093/cercor/bhl147
*   Jolliffe, I. T. (2002). *Principal component analysis*. Springer.
*   Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. *Journal of Basic Engineering*, *82*(1), 35-45.
*   Kandel, E. R. (2001). The molecular biology of memory storage: a dialogue between genes and synapses. *Science*, *294*(5544), 1030-1038.
*   Kaner, C., Falk, J., & Nguyen, H. Q. (1999). *Testing computer software*. John Wiley & Sons.
*   Karypis, G., & Kumar, V. (1998). A fast and high quality multilevel scheme for partitioning irregular graphs. *SIAM Journal on Scientific Computing*, *20*(1), 359-392.
*   Khalil, H. K. (2002). *Nonlinear systems* (3rd ed.). Prentice Hall.
*   Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with graph convolutional networks. In *International Conference on Learning Representations (ICLR)*.
*   Knight, J. C. (2000). Safety critical systems: challenges and directions. In *Proceedings of the 22nd international conference on Software engineering* (pp. 547-550).
*   Kullback, S., & Leibler, R. A. (1951). On information and sufficiency. *The Annals of Mathematical Statistics*, *22*(1), 79-86.
*   Lamport, L. (1998). The part-time parliament. *ACM Transactions on Computer Systems (TOCS)*, *16*(2), 133-169.
*   Lapicque, L. (1907). Recherches quantitatives sur l'excitation électrique des nerfs traitée comme une polarisation. *Journal de Physiologie et de Pathologie Générale*, *9*, 620-635.
*   Larson, J., & Lynch, G. (1986). Induction of synaptic potentiation in hippocampus by patterned stimulation involves two events. *Science*, *232*(4753), 985-988.
*   Laughlin, S. B., & Sejnowski, T. J. (2003). Communication in neuronal networks. *Science*, *301*(5641), 1870-1874.
*   Law, A. M. (2015). *Simulation modeling and analysis* (5th ed.). McGraw-Hill Education.
*   Lietz, J. (2025). *How the Fully Unified Model (FUM) Works*. [Unpublished technical specification / Design document]. (Details the specific FUM architecture and the following potentially novel contributions:
    *   **Overall Integrated System:** The synergistic combination of SNNs, the Self-Improvement Engine, detailed structural plasticity, emergent knowledge graph, hybrid computation model, and phased training strategy as a unified system.
    *   **Self-Improvement Engine (SIE):** The specific reward formulation `total_reward = TD_error + novelty - habituation + self_benefit`, including the `self_benefit = complexity * impact` calculation, and its direct, quadratically scaled modulation of STDP learning rates via eligibility traces.
    *   **Integrated Structural Plasticity System:** The specific set of triggers (low cluster reward, high variance, low neuron activity), detailed algorithms (targeted growth, pruning with homeostatic compensation, co-activation-based rewiring), and defined operational limits (e.g., connection history, sparsity target, E/I balancing during rewiring).
    *   **Adaptive Domain Clustering Integration:** Employing K-Means with dynamic `k` selection (via Silhouette Score within defined bounds) to define TD-Learning states and guide reward attribution and structural plasticity within the SNN framework, including specific edge case handling.
    *   **Multi-Phase Training Strategy:** The explicit three-phase approach (Seed Sprinkling -> Tandem Complexity Scaling -> Continuous Self-Learning) tailored for minimal data dependency and autonomous operation.
    *   **Emergent Knowledge Graph & Routing Mechanism:** Reliance on learned SNN connectivity dynamically shaped by STDP/SIE/plasticity for coordination and information routing, explicitly contrasting with predefined layers or coordinators.
    *   **Emergent Energy Landscape Concept:** Proposing network stability as arising naturally from the interplay of local/global rules (measured via variance), rather than an explicitly defined energy function.
    *   **Specific Hybrid Computation Model:** The defined heterogeneous GPU workload distribution (LIF kernel vs. PyTorch/SIE/Clustering) and associated data flow/synchronization strategy.
    *   **Specific Temporal Encoding Schemes:** The described hierarchical methods for encoding structured data (e.g., code syntax trees, logical propositions) into temporal spike patterns.
    *   **Minimal Data Philosophy:** The core design goal targeting high performance from minimal inputs (80-300) based on a defined balance between initialization and learning.)
*   Lisman, J. E., Grace, A. A., & Duzel, E. (2011). A neoHebbian framework for episodic memory; role of dopamine-dependent late LTP. *Trends in Neurosciences*, *34*(10), 536-547.
*   Liu, C. L., & Layland, J. W. (1973). Scheduling algorithms for multiprogramming in a hard-real-time environment. *Journal of the ACM (JACM)*, *20*(1), 46-61.
*   Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. In *2008 Eighth IEEE International Conference on Data Mining* (pp. 413-422). IEEE.
*   London, M., & Häusser, M. (2005). Dendritic computation. *Annual Review of Neuroscience*, *28*, 503-532.
*   Maass, W. (1997). Networks of spiking neurons: the third generation of neural network models. *Neural Networks*, *10*(9), 1659-1671.
*   MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. In *Proceedings of the fifth Berkeley symposium on mathematical statistics and probability* (Vol. 1, pp. 281-297). University of California Press.
*   Marder, E. (2012). Neuromodulation of neuronal circuits: back to the future. *Neuron*, *76*(1), 1-11.
*   Marder, E., & Goaillard, J. M. (2006). Variability, compensation, and homeostasis in neuron and network function. *Nature Reviews Neuroscience*, *7*(7), 563-574.
*   Markram, H., Gerstner, W., & Sjöström, P. J. (2011). Spike-timing-dependent plasticity: a learning rule for the brain?. *Frontiers in Synaptic Neuroscience*, *4*, 2.
*   Markram, H., Lübke, J., Frotscher, M., & Sakmann, B. (1997). Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs. *Science*, *275*(5297), 213-215.
*   Marsland, S. (2014). *Machine learning: an algorithmic perspective*. CRC press.
*   McCloskey, M., & Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The sequential learning problem. *Psychology of learning and motivation*, *24*, 109-165.
*   Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A survey on bias and fairness in machine learning. *ACM Computing Surveys (CSUR)*, *54*(6), 1-35.
*   Merkel, D. (2014). Docker: Lightweight Linux Containers for Consistent Development and Deployment. *Linux Journal*, *2014*(239), 2.
*   Metropolis, N., & Ulam, S. (1949). The monte carlo method. *Journal of the American statistical association*, *44*(247), 335-341.
*   Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. *arXiv preprint arXiv:1301.3781*.
*   Mitchell, M. (2009). *Complexity: A guided tour*. Oxford University Press.
*   Mountcastle, V. B. (1997). The columnar organization of the neocortex. *Brain*, *120*(4), 701-722.
*   Murphy, K. P. (2002). *Dynamic Bayesian networks: representation, inference and learning*. University of California, Berkeley.
*   Myers, G. J., Sandler, C., & Badgett, T. (2011). *The art of software testing*. John Wiley & Sons.
*   Näätänen, R., Paavilainen, P., Rinne, T., & Alho, K. (2007). The mismatch negativity (MMN) in basic research of central auditory processing: A review. *Clinical Neurophysiology*, *118*(12), 2544-2590.
*   Nakazawa, K., Quirk, M. C., Chitwood, R. A., Watanabe, M., Yeckel, M. F., Sun, L. D., ... & Tonegawa, S. (2002). Requirement for hippocampal CA3 NMDA receptors in associative memory recall. *Science*, *297*(5579), 211-218.
*   Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. In *Proceedings of the Sixteenth International Conference on Machine Learning (ICML)* (pp. 278-287).
*   Ongaro, D., & Ousterhout, J. (2014). In search of an understandable consensus algorithm. In *2014 USENIX Annual Technical Conference (USENIX ATC 14)* (pp. 305-319).
*   Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In *Advances in Neural Information Processing Systems 32* (pp. 8026-8037). Curran Associates, Inc.
*   Pearl, J. (2009). *Causality*. Cambridge University Press.
*   Pfeiffer, M., & Pfeil, T. (2018). Deep learning with spiking neurons: opportunities and challenges. *Frontiers in Neuroscience*, *12*, 774. https://doi.org/10.3389/fnins.2018.00774
*   Pinedo, M. L. (2016). *Scheduling: theory, algorithms, and systems*. Springer.
*   Poo, M. M. (2001). Neurotrophins as synaptic modulators. *Nature Reviews Neuroscience*, *2*(1), 24-32.
*   Puterman, M. L. (1994). *Markov decision processes: Discrete stochastic dynamic programming*. John Wiley & Sons.
*   Rakic, P. (1988). Specification of cerebral cortical areas. *Science*, *241*(4862), 170-176.
*   Rausand, M., & Høyland, A. (2004). *System reliability theory: models, statistical methods, and applications*. John Wiley & Sons.
*   Reynolds, D. A. (2009). Gaussian Mixture Models. In *Encyclopedia of Biometrics*. Springer US.
*   Rice, J. A. (2007). *Mathematical Statistics and Data Analysis*. Duxbury Press.
*   Roelfsema, P. R., & Holtmaat, A. (2018). Control of synaptic plasticity in deep cortical networks. *Nature Reviews Neuroscience*, *19*(3), 166-180.
*   Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. *Journal of Computational and Applied Mathematics*, *20*, 53-65.
*   Rubino, G., & Tuffin, B. (Eds.). (2009). *Rare event simulation using Monte Carlo methods*. John Wiley & Sons.
*   Saltelli, A., Ratto, M., Andres, T., Campolongo, F., Cariboni, J., Gatelli, D., Saisana, M., & Tarantola, S. (2008). *Global sensitivity analysis: The primer*. John Wiley & Sons.
*   Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2009). The graph neural network model. *IEEE Transactions on Neural Networks*, *20*(1), 61-80.
*   Scheffer, M., Bascompte, J., Brock, W. A., Brovkin, V., Carpenter, S. R., Dakos, V., ... & Sugihara, G. (2009). Early-warning signals for critical transitions. *Nature*, *461*(7260), 53-59.
*   Schultz, W. (1998). Predictive reward signal of dopamine neurons. *Journal of Neurophysiology*, *80*(1), 1-27.
*   Shalev-Shwartz, S. (2012). Online learning and online convex optimization. *Foundations and Trends® in Machine Learning*, *4*(2), 107-194.
*   Shannon, C. E. (1948). A mathematical theory of communication. *The Bell System Technical Journal*, *27*(3), 379-423.
*   Šiljak, D. D. (1991). *Decentralized control of complex systems*. Academic Press.
*   Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In *Advances in Neural Information Processing Systems 25* (pp. 2951-2959). Curran Associates, Inc.
*   Song, S., Miller, K. D., & Abbott, L. F. (2000). Competitive Hebbian learning through spike-timing-dependent synaptic plasticity. *Nature Neuroscience*, *3*(9), 919-926.
*   Squire, L. R. (1992). Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans. *Psychological review*, *99*(2), 195.
*   Strogatz, S. H. (2015). *Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering*. Westview Press.
*   Stuart, G. J., & Spruston, N. (2015). Dendritic integration: 60 years of progress. *Nature Neuroscience*, *18*(12), 1713-1721.
*   Sur, M., & Rubenstein, J. L. (2005). Patterning and plasticity of the cerebral cortex. *Science*, *310*(5749), 805-810.
*   Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. *Machine Learning*, *3*(1), 9-44.
*   Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
*   Tanenbaum, A. S., & Van Steen, M. (2007). *Distributed systems: Principles and paradigms*. Prentice Hall.
*   Thorpe, S., Delorme, A., & Van Rullen, R. (2001). Spike-based strategies for rapid processing. *Neural Networks*, *14*(6-7), 715-725.
*   Torrey, L., & Shavlik, J. (2010). Transfer learning. In *Handbook of research on machine learning applications and trends: algorithms, methods, and techniques* (pp. 242-264). IGI global.
*   Triesch, J. (2007). Synergies between intrinsic and synaptic plasticity mechanisms. *Neural Computation*, *19*(4), 885-909.
*   Turrigiano, G. G., & Nelson, S. B. (2004). Homeostatic plasticity in the developing nervous system. *Nature Reviews Neuroscience*, *5*(2), 97-107.
*   Turrigiano, G. G., Leslie, K. R., Desai, N. S., Rutherford, L. C., & Nelson, S. B. (1998). Activity-dependent scaling of quantal amplitude in neocortical neurons. *Nature*, *391*(6670), 892-896.
*   Vapnik, V. (1998). *Statistical learning theory*. Wiley.
*   Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., & Gerstner, W. (2011). Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks. *Science*, *334*(6062), 1569-1573. https://doi.org/10.1126/science.1211095
*   Watts, D. J. (2002). A simple model of global cascades on random networks. *Proceedings of the National Academy of Sciences*, *99*(9), 5766-5771.
*   Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In *International Conference on Learning Representations (ICLR)*.
