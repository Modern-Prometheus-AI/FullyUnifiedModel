# FUM Glossary (Generated from How_It_Works Sections 0-7)

*   **Adaptive Domain Clustering:** The process in FUM of dynamically grouping neurons based on similar firing rate profiles using k-means clustering (with dynamically selected `k` via silhouette score). This identifies emergent functional specializations (domains) and provides the state representation for the TD learning value function (`V_states`). (See Section 2.F, 4.D)
*   **ANN (Artificial Neural Network):** Conventional neural network models (e.g., CNNs, RNNs, Transformers) typically using rate-based units (like ReLU), fixed layered architectures, and learning rules like backpropagation. Contrasted with FUM's SNN approach. (See Section 1.C, 2.A.2)
*   **Asynchronous Updates:** A scaling strategy where different partitions (shards) of the FUM network across GPUs/nodes can simulate slightly out-of-sync (within a defined skew tolerance, e.g., 1ms) to improve throughput, managed using timestamps, buffering, and periodic synchronization. (See Section 5.D.2)
*   **BDNF (Brain-Derived Neurotrophic Factor) Proxy:** An activity-dependent variable (`bdnf_proxy[i] = spike_rate[i] / target_rate`) used as an enhanced trigger for structural plasticity (growth), mimicking the role of biological growth factors. (See Section 4.C.2)
*   **Catastrophic Forgetting:** The tendency for neural networks to abruptly lose previously learned knowledge when learning new information. FUM aims to mitigate this through mechanisms like synaptic decay, persistence tags, and structural plasticity stability checks. (See Section 5.E.4)
*   **Causal Inference:** Used in FUM to refine credit assignment by estimating the true causal contribution of a cluster or pathway to an outcome, often involving approximations like linear models for interventions, aiming to prevent reward hacking. (See Section 2.C.8, 5.E.6)
*   **Causal Inference Engine:** A component of the Unified Debugging Framework that uses techniques like Bayesian networks to infer causal relationships in the emergent knowledge graph, helping to pinpoint root causes of failures. (See Section 5.E.8, 5.E.11)
*   **Clock Skew / Jitter:** Timing variations between different components or nodes in the distributed system. FUM aims to manage this using high-precision synchronization (PTP) and mechanisms like temporal integration to maintain STDP accuracy within a target skew tolerance (e.g., 1ms). (See Section 5.A.2, 5.D.2, 5.E.5)
*   **Cluster / Clustering:** A group of neurons identified by Adaptive Domain Clustering as having similar activity profiles, representing an emergent functional specialization or domain. Used for state representation in TD learning and guiding plasticity. (See Section 2.F, 4.D)
*   **Complexity as Strength:** The principle that FUM's intricate, emergent complexity—quantified by metrics like Integrated Information Theory (IIT) Φ values (~20 bits @ 5B neurons) and fractal dimension (~3.4 @ 5B neurons)—is not a flaw but a source of functional advantages, including adaptability, computational efficiency, and enhanced reasoning depth (+30-35% @ 5B neurons), validated empirically at scale. (See Section 4.K, 6.D)
*   **Compositionality:** The ability of FUM to combine learned primitives (basic functions represented by clusters or pathways) to perform more complex, multi-step computations or reasoning, emerging from cross-cluster STDP and SIE reinforcement. (See Section 2.D.4)
*   **Consolidation:** The process of strengthening and stabilizing learned knowledge (represented by synaptic weights and pathways) for long-term retention, managed in FUM via mechanisms like persistence tags and synaptic scaling. (See Section 5.E.4)
*   **Continuous Reinforcement Learning:** The learning paradigm used by FUM's Self-Improvement Engine (SIE), where a continuous reward signal guides learning based on performance and internal state, often without explicit labels. (See Section 2.C.1)
*   **Control Impact:** A metric (`control_FLOPs / system_FLOPs`) used in FUM to quantify the computational overhead of control mechanisms relative to the core simulation, ensuring control remains minimal guidance (< 1e-5 target) to preserve emergent dynamics. (See Section 5.E.7)
*   **Critical Period (Analogue):** A simulated period early in FUM's training (Phase 1) where structural plasticity rates (growth, rewiring) are temporarily increased to facilitate rapid initial structure formation, mimicking developmental windows in biology. (See Section 4.C.2)
*   **Data Curriculum:** The strategy in FUM's Phase 2 training of sequentially introducing batches of data with increasing complexity to refine the network and build competence gradually. (See Section 5.B.2)
*   **Decoder / Decoding:** The mechanism in FUM that translates the spiking activity of designated output neurons back into a human-understandable format (e.g., text, classification, numerical value) using methods like rate or temporal decoding. (See Section 3.B)
*   **Distributed Computation / Sharding:** The scaling strategy of partitioning the FUM network (neurons and connections) across multiple GPUs or compute nodes using graph partitioning algorithms like METIS to enable simulation of massive networks. (See Section 5.D.1)
*   **Distributed Lock:** A mechanism used during structural modifications in the distributed FUM system to prevent race conditions by ensuring only one process modifies the shared structure at a time. (See Section 5.D.2)
*   **Dynamic Ethics Adjuster:** A mechanism within the SIE that dynamically weights ethical constraints in the reward signal based on context and violation detection, ensuring adaptive ethical alignment. (See Section 2.C.9, 6.G.2)
*   **Eligibility Trace (`e_ij`):** A synapse-specific variable that decays over time (`γ`) and accumulates recent STDP weight changes (`Δw_ij`). It allows a potentially delayed global reward signal (from SIE) to correctly modify synapses based on their past contribution to relevant spike timings (temporal credit assignment). FUM may use enhancements like variable decay or an STC analogue. (See Section 2.B.5)
*   **Emergence / Emergent:** The principle that complex behaviors, structures (like the knowledge graph), or properties (like stability) arise spontaneously from the interaction of simpler, local rules (LIF dynamics, STDP, inhibition) and global feedback (SIE) without being explicitly programmed for those specific outcomes. FUM's emergent structures are validated empirically for reliability (e.g., >90% emergence preservation rate). (See Section 1.B, 2.D, 4.A, 4.B.3, 6.A.2)
*   **Emergent Energy Landscape:** The concept in FUM that network stability (a low-energy state, measured by low firing rate variance) emerges naturally from the interplay of learning rules and feedback, rather than being defined by a fixed mathematical function. (See Section 4.A)
*   **Emergent Knowledge Graph:** The dynamic graph structure in FUM formed by the learned synaptic connections (`w_ij`) between neurons. It evolves through STDP, SIE, and structural plasticity, serving as a distributed associative memory and reasoning substrate where relationships emerge from data. (See Section 1.B.4, 2.D, 4.B)
*   **Encoder / Encoding:** The mechanism in FUM that translates raw input data from various modalities (text, images, etc.) into the universal format of temporal spike trains processed by the SNN core, using methods like hierarchical or spike pattern encoding. (See Section 3.A)
*   **Exaptation (Analogue):** A mechanism in FUM where existing, successful pathways or clusters are duplicated or repurposed to initialize structures for new domains or functions, accelerating learning by leveraging established components. (See Section 2.B.8)
*   **Excitatory Neuron / Synapse:** Neurons that, when firing, tend to increase the membrane potential of post-synaptic neurons (positive `w_ij`). FUM typically uses an 80:20 ratio of excitatory to inhibitory neurons. (See Section 1.B.3, 2.B.2)
*   **Expert-Level Mastery:** The target performance level for FUM, defined by high accuracy (>85-90%) on specific complex subsets of benchmarks (e.g., MATH, GPQA, HumanEval) and emergent validation tests after training on minimal data (80-300 inputs). (See Section 1.A.7)
*   **Failure Impact Model:** A model using techniques like Fault Tree Analysis (FTA) to quantify the potential negative consequences (impact) associated with different failure modes identified by the Probabilistic Failure Model. Informs risk assessment. (See Section 6.F)
*   **Fault Tolerance:** The ability of the distributed FUM system to continue operating despite hardware failures (nodes, memory errors) or network partitions, achieved through mechanisms like consensus algorithms (Raft), redundancy, checkpointing, and ECC memory. (See Section 5.D.3, 5.D.4, 6.A.1)
*   **Fractal Dynamics / Fractal Intelligence Hypothesis:** The hypothesis and observation that FUM's network activity and emergent structures exhibit fractal properties (self-similarity across scales, e.g., dimension ~3.4 @ 5B neurons), which correlates with enhanced reasoning depth and efficient information processing. (See Section 4.K.3)
*   **Formal Methods / Guarantees:** Mathematical techniques (e.g., Lyapunov stability analysis, causal inference, model checking, spectral graph theory) applied, often with approximations, to provide theoretical confidence in FUM's stability, correctness, and alignment, complemented by brain-inspired validation metrics. (See Section 2.C.8, 5.E.6, 6.A.8)
*   **FUM (Fully Unified Model):** The specific AI architecture detailed in the documentation, characterized by its use of spiking neurons (LIF), STDP, an emergent knowledge graph, multiple forms of plasticity (intrinsic, structural), a Self-Improvement Engine (SIE) for reinforcement learning, and a focus on data efficiency and emergent intelligence. (See Section 0, 1.A)
*   **Functional Specialization:** The process by which different groups of neurons or clusters in FUM become selectively responsive to specific types of inputs or involved in particular computations, emerging primarily through activity-dependent self-organization (STDP, inhibition, plasticity) potentially guided by weak initial connectivity priors. (See Section 2.D.3, 4.E)
*   **Gaming / Reward Hacking:** The risk that the system learns to maximize the internal SIE reward signal through unintended means that don't correspond to desired external task performance. FUM employs safeguards like reward capping, normalization, ground truth injection, diversity monitoring, and robust reward design to prevent this. (See Section 2.C.8, 5.E.4)
*   **Generalization:** The ability of FUM to perform well on new, unseen inputs or tasks that differ from the training data, indicating a deeper understanding rather than just memorization. FUM's validation emphasizes testing generalization using emergent synthetic data and curated real-world examples. (See Section 1.A.3, 5.E.8)
*   **GNN (Graph Neural Network):** Neural network models designed to operate directly on graph-structured data. Contrasted with FUM, where the graph structure itself emerges from learning. (See Section 2.D.1)
*   **Habituation (SIE component):** A component of the `total_reward` signal that reduces the reward for frequently encountered input patterns, discouraging overfitting/memorization and promoting exploration of novel stimuli. Calculated based on similarity to recent inputs. (See Section 2.C.5)
*   **Hardware Agnosticism:** The design principle that FUM's core algorithms (LIF, STDP, SIE) are independent of specific hardware, allowing potential implementation across various platforms, although specific optimizations might be used for development or deployment. (See Section 5.D.5)
*   **Heterogeneity (Neuron Parameters):** The intentional variation in parameters (like `tau_i`, `v_th_i`) across neurons, drawn from distributions at initialization. This mimics biological variability and enhances network dynamics by preventing excessive synchronization. (See Section 2.A.5)
*   **Homeostasis / Homeostatic Plasticity:** Biological principle of maintaining stable internal conditions. In FUM, refers to mechanisms like intrinsic plasticity and synaptic scaling that regulate neuron firing rates and synaptic strengths to keep network activity within a functional range, contributing to overall stability. (See Section 2.A.6, 2.B.7, 4.C.2, 5.E.7)
*   **Hybrid Architecture / Interface:** FUM's approach combining SNN simulation (often in custom kernels on one GPU type, e.g., 7900 XTX) for core neural dynamics with tensor-based computation (using libraries like PyTorch on another GPU type, e.g., MI100) for overhead tasks like SIE calculation, clustering, and trace updates, linked via a defined data flow and synchronization protocol. (See Section 1.B.1, 2.E)
*   **Hyperparameter Tuning:** The process of finding optimal values for model parameters not learned directly from data (e.g., `eta`, `gamma`, SIE weights). FUM uses automated Bayesian optimization to tune sensitive hyperparameters. (See Section 5.E.1)
*   **Impact (SIE component):** A (now deprecated) component previously part of the `self_benefit` calculation, intended to measure the functional effect of network activity. Replaced by a homeostasis-based metric. (See Section 2.C.6)
*   **Inhibitory Neuron / Synapse / STDP:** Neurons that decrease the membrane potential of post-synaptic neurons (negative `w_ij`). They play crucial roles in balancing excitation, stabilizing network activity, segregating functional clusters, and enabling computations like negation. Inhibitory STDP rules differ from excitatory ones to promote stability. (See Section 1.B.3, 2.B.3, 2.B.7, 2.D.3, 4.E.1, 5.E.7)
*   **Integrated Information Theory (IIT):** A theoretical framework used in FUM to quantify the degree of irreducible cause-effect power (Φ value) within the system, hypothesized to correlate with integrated reasoning capabilities (e.g., Φ ~20 bits @ 5B neurons). (See Section 4.K.1)
*   **Initialization / Seed Sprinkling (Phase 1):** The first phase of FUM training. Establishes a sparse, foundational network structure using minimal diverse data (80 inputs), distance-biased connectivity, and weak random weights, preparing the network for further learning. (See Section 5.A, 6.B.1)
*   **Intrinsic Plasticity:** A homeostatic mechanism where individual neuron parameters (like firing threshold `v_th_i` and membrane time constant `tau_i`) adapt based on the neuron's recent firing rate to maintain activity within a target range (e.g., 0.1-0.5 Hz). (See Section 2.A.6, 2.B.7, 5.E.4)
*   **Junk Data Injection Testing:** A validation technique where irrelevant or nonsensical data is intentionally added during training or testing to assess the model's robustness against overfitting and its ability to discern meaningful patterns. (See Section 6.A.7)
*   **K-Means Clustering:** An algorithm used in FUM's Adaptive Domain Clustering to group neurons into `k` clusters based on minimizing the distance between neurons' firing rate profiles and cluster centroids. (See Section 2.F.1, 5.E.6)
*   **Lamarckian (Analogy):** Refers to the aspect of FUM's learning where adaptations acquired through experience (via reward-modulated STDP and structural plasticity) directly modify the network structure, resembling the (biologically largely discredited) theory of inheritance of acquired characteristics. FUM includes safeguards to ensure long-term adaptiveness. (See Section 2.B.9)
*   **Latency:** The time delay in transmitting information (e.g., spikes) between different parts of the distributed FUM system. Managed through timestamp correction and potentially adaptive STDP windows. (See Section 5.D.2, 5.E.5)
*   **LIF (Leaky Integrate-and-Fire):** The specific spiking neuron model used in FUM. It models a neuron's membrane potential (`V`) integrating input currents (`I`), leaking charge over time (`tau`), firing a discrete spike when `V` reaches a threshold (`v_th`), and then resetting (`v_reset`). Chosen for its balance of biological plausibility and computational efficiency. (See Section 1.B.1, 2.A)
*   **LLM (Large Language Model):** Models like GPT-3/4, typically based on Transformer architectures, trained on massive text datasets using supervised learning (pre-training) and known for broad knowledge but high data/energy costs. Contrasted with FUM's approach. (See Section 1.A.2, 1.B.4, 1.C.1, 2.D.1, 3.A.1, 5.E.3, 6.A.5)
*   **Local Learning Rule:** A learning rule where synaptic weight changes depend only on information available locally at the synapse (e.g., pre- and post-synaptic activity). STDP is a key example in FUM, contrasted with global rules like backpropagation. (See Section 2.B.1, 6.A.2)
*   **Markov Property:** The assumption that the future state of a system depends only on the current state, not on the sequence of events that preceded it. FUM uses cluster IDs as an approximation of a Markov state for its TD learning value function. (See Section 2.C.3)
*   **Memorization:** Learning specific input-output pairs from the training data without understanding the underlying patterns, leading to poor performance on unseen data. FUM aims to avoid this through minimal data, SIE mechanisms (novelty, habituation), sparsity, and specific validation tests. (See Section 1.A.3, 5.E.8)
*   **METIS:** A graph partitioning library used in FUM's scaling strategy to divide the neuron graph across distributed compute nodes while minimizing connections (communication) between partitions. (See Section 5.D.1)
*   **Minimal Data:** FUM's core philosophy and goal of achieving expert-level mastery using a very small number of training examples (target: 80-300 inputs), relying on efficient learning mechanisms (STDP/SIE) and emergent generalization rather than massive datasets. (See Section 1.A, 1.A.10, 6.B.1)
*   **Modularity:** The property of a system being composed of distinct functional units (modules). In FUM, modularity emerges through Adaptive Domain Clustering, which identifies functionally specialized groups of neurons. (See Section 2.D.4, 5.E.7)
*   **Neuromodulation / Neuromodulatory Effects (Analogue):** Biological process where chemicals (like dopamine) broadly influence neuronal activity and plasticity. FUM's SIE reward signal acts as a simplified global analogue, with enhancements like cluster-specific rewards or distinct signal components (e.g., dopamine/acetylcholine proxies) aiming for more targeted, brain-inspired modulation. (See Section 2.A.1, 2.B.4, 2.C.2)
*   **Neuron Parameters (`tau`, `v_th`, `v_reset`):** Key parameters defining the behavior of an LIF neuron: `tau` (membrane time constant, affecting leak), `v_th` (firing threshold), and `v_reset` (resting potential after firing). FUM uses heterogeneous values for `tau` and `v_th`. (See Section 2.A.3, 2.A.5, 2.A.6)
*   **Neutral Drift / Rewiring (Analogue):** A mechanism inspired by genetic drift, allowing small, random synaptic changes or rewiring even when performance is stable, enabling exploration of functionally equivalent network configurations without immediate reward pressure. (See Section 2.B.8, 2.B.9)
*   **Novelty (SIE component):** A component of the `total_reward` signal that rewards the processing of new, previously unseen input patterns (measured by similarity to recent inputs), encouraging exploration and adaptation. (See Section 2.C.4, 2.C.8, 4.C.2, 5.E.4)
*   **OOD (Out-of-Distribution) Testing:** Evaluating model performance on data drawn from a different distribution than the training data, used as one method to assess generalization. (See Section 1.A.3, 5.E.8, 6.A.9)
*   **Parameter Server:** A distributed systems pattern used in FUM's scaling strategy where large model parameters (like the sparse weight matrix `w`) are sharded across the memory of multiple nodes, and compute nodes fetch needed parameters as required. (See Section 5.D.4)
*   **Pathway Protection / Persistence Tag:** A mechanism in FUM to protect important, consolidated knowledge. Synapses belonging to consistently high-reward pathways are marked as "persistent" and are exempted from synaptic decay and potentially disruptive structural changes (like rewiring). Dynamic thresholds manage tagging and de-tagging to balance stability and adaptability. (See Section 2.D.3, 2.D.4, 4.C.3, 5.E.4, 6.A.6)
*   **Phase Transition Predictor:** A component extending the Scaling Dynamics Model, using bifurcation analysis to identify critical parameter thresholds where FUM's behavior might undergo abrupt shifts, allowing for proactive mitigation during scaling. (See Section 2.I)
*   **Phase 1 / 2 / 3 (Training):** FUM's multi-phase training strategy: Phase 1 (Random Seed Sprinkling) builds a foundation from minimal data; Phase 2 (Tandem Complexity Scaling) refines the network using a curriculum; Phase 3 (Continuous Self-Learning) involves autonomous operation and adaptation on continuous data streams. (See Section 5.A, 5.B, 5.C)
*   **Plasticity (Neural / Synaptic / Structural):** The ability of the network to change. Includes synaptic plasticity (STDP changing weights `w_ij`), intrinsic plasticity (adapting neuron parameters `tau_i`, `v_th_i`), and structural plasticity (adding/removing neurons/connections). (See Section 1.C.5, 2.A.6, 2.B, 4.C, 6.A.4)
*   **Predictive Debugging Model:** A component of the Unified Debugging Framework that uses predictive methods (e.g., reinforcement learning) to anticipate potential failure modes based on network state, enabling proactive intervention. (See Section 5.E.9, 5.E.11)
*   **Poisson Spike Generation:** The method used in FUM's encoder (and potentially internally) to generate stochastic spike trains, where the probability of a spike occurring in a small time interval is proportional to a target firing rate (`f`). (See Section 3.A.3)
*   **Probabilistic Failure Model:** A model using techniques like Monte Carlo simulation to estimate the probability of different failure modes occurring during FUM's development and scaling, informing risk assessment. (See Section 6.E)
*   **PTP (Precision Time Protocol):** A network protocol (IEEE 1588) used in FUM's distributed implementation to achieve high-precision clock synchronization (nanosecond to microsecond level) across nodes, crucial for maintaining the timing accuracy required by STDP. (See Section 5.A.2, 5.D.2, 5.E.5)
*   **Raft (Consensus Algorithm):** A distributed consensus algorithm used in FUM's control plane to manage state and handle node failures reliably in the distributed system. (See Section 5.D.3, 6.A.10)
*   **Rate Coding / Decoding:** Representing information by the average firing rate of neurons over a time window. Used as one method in FUM's encoder and decoder, often for simpler inputs/outputs. (See Section 3.A.2, 3.B.2)
*   **Refractory Period:** A brief period after a neuron fires during which it cannot fire again (or has reduced excitability). FUM implements a 5ms absolute refractory period in its LIF model and input encoding. (See Section 2.A.4, 3.A.3)
*   **Reinforcement Learning (RL):** A machine learning paradigm where an agent learns by receiving rewards or penalties for its actions. FUM uses RL principles via the SIE. (See Section 1.C.4, 2.C)
*   **Reliability:** The consistency and correctness of FUM's operations, including primitive formation, routing, and long-range dependencies. Ensured through mechanisms like SIE guidance, inhibition, stability controls, and validation. (See Section 2.B.2, 2.B.7, 2.D.3, 6.A.6)
*   **Resource Efficiency Protocol:** A set of strategies employed by FUM to minimize computational resource consumption (GPU time, energy) during development and operation, including optimized kernels, efficient data handling, and dynamic resource allocation. (See Section 6.G.3)
*   **Reward Signal (`total_reward`, `r`):** The feedback signal used in FUM's reinforcement learning. `r` is an immediate external reward (if available), while `total_reward` is the internally calculated SIE signal combining TD error, novelty, habituation, and self-benefit to guide STDP. (See Section 2.C.2, 2.C.8, 5.E.4)
*   **ROCm / HIP:** AMD's software platform and C++ runtime API for GPU computing, used in FUM for writing and executing custom, high-performance kernels (e.g., for the LIF simulation loop) on AMD GPUs (like 7900 XTX, MI100). (See Section 2.E.2, 5.D.5)
*   **Routing (Graph):** The process by which information (propagating spike activity) flows through the emergent knowledge graph along pathways determined by learned synaptic strengths (`w_ij`). (See Section 2.D.4)
*   **Scaling Dynamics Model:** A model utilizing dynamical systems theory to analyze feedback loops (e.g., STDP-SIE-plasticity) and predict how FUM's stability and performance metrics evolve as the network scales, guiding development. (See Section 2.H)
*   **Self-Benefit (SIE component):** A component of the `total_reward` signal designed to promote stable and efficient network operation, analogous to biological homeostasis. Calculated based on the deviation of firing rate variance from a target value. (See Section 2.C.6)
*   **Self-Improvement Engine (SIE):** A core FUM component that calculates the `total_reward` signal based on TD error, novelty, habituation, and self-benefit. This global reward signal modulates the local STDP learning rate via eligibility traces, guiding the network's self-organization towards desired outcomes. (See Section 1.A.2, 1.B.1, 1.C.4, 2.C, 6.A.2)
*   **Self-Modification:** See Structural Plasticity.
*   **Self-Organized Criticality (SOC):** A state observed in some complex systems (including potentially the brain) characterized by a balance between stability and chaotic fluctuations, often exhibiting power-law distributions (e.g., neuronal avalanches). FUM aims to operate near SOC, managed by mechanisms like predictive avalanche control and dynamic inhibition, to enhance information processing. (See Section 4.A.3, 5.C.3, 5.E.7)
*   **Semantic Coverage:** A metric used during FUM's initial data curation to ensure the minimal input set adequately represents the key concepts within each target domain, often measured using embedding similarity. (See Section 5.A.2)
*   **Sensitivity Analysis:** Techniques used to assess how changes in model parameters or assumptions affect the system's behavior or performance, helping to identify critical parameters and ensure robustness. (See Section 2.C.8, 5.E.1, 5.E.6)
*   **Silhouette Score:** A metric used in FUM's Adaptive Domain Clustering to evaluate the quality of clustering for different values of `k` (number of clusters) and select the optimal `k`. It measures how similar an object is to its own cluster compared to other clusters. (See Section 2.F.2, 5.B.3)
*   **Simplicity (Design Principle):** Refers to the conceptual elegance and minimalism of FUM's core operating principles (e.g., local rules driving emergence, minimal control impact), as distinct from the necessary complexity of its implementation which involves numerous interacting components to realize these principles effectively and ensure stability. (See Section 1.B.2, 6.B.2)
*   **SNN (Spiking Neural Network):** Neural networks composed of spiking neurons (like LIF) that communicate using discrete events (spikes) over time. FUM is based on SNNs, leveraging their potential for temporal processing and energy efficiency. (See Section 1.A.2, 1.B.3, 6.A.1)
*   **Sparsity:** The property of having only a small fraction of elements being non-zero. FUM targets high sparsity (~95%) in its synaptic connections (`w`) for computational and memory efficiency, and also leverages sparse spiking activity. (See Section 1.A.2, 1.B.3, 2.D.2, 5.A.2, 5.E.3, 6.A.1)
*   **Spike / Spiking:** The discrete, event-based signal used for communication between neurons in SNNs. Information is encoded in the timing and patterns of spikes. (See Section 2.A.4)
*   **Spike Pattern Encoding:** An enhanced encoding method in FUM that uses the precise timing of spikes within a window, not just the rate, to represent input features, increasing information capacity. (See Section 3.A.2)
*   **Spike Pathway Tracing:** A debugging and interpretability technique in FUM to reconstruct the sequence of spike propagation through the network for a given computation, helping to understand the reasoning process. (See Section 5.E.2)
*   **Spike Timing:** The precise moment when a neuron fires a spike. Crucial for STDP and temporal coding in FUM. (See Section 1.B.3, 2.B.1, 5.E.5)
*   **STC (Synaptic Tagging and Capture) Analogue:** An enhanced mechanism for eligibility traces in FUM, inspired by the biological process. It involves "tagging" synapses undergoing significant potentiation and consolidating them over longer timescales, potentially improving long-term memory and interference prevention. (See Section 2.B.5)
*   **STDP (Spike-Timing-Dependent Plasticity):** FUM's primary synaptic learning rule. The change in synaptic weight (`Δw_ij`) between two neurons depends exponentially on the precise time difference (`Δt`) between their spikes, typically strengthening connections when the pre-synaptic neuron fires just before the post-synaptic one (potentiation) and weakening them in the reverse case (depression). Different rules apply for excitatory and inhibitory synapses. (See Section 1.A.2, 1.C.2, 2.B, 6.A.2)
*   **STDP Parameters (`A_+`, `A_-`, `τ_+`, `τ_-`, `eta`):** Parameters controlling the magnitude (`A_+`, `A_-`) and time course (`τ_+`, `τ_-`) of STDP weight changes, and the base learning rate (`eta`). FUM may use constrained variability in these parameters. (See Section 2.B.4)
*   **Structural Plasticity (Growth, Pruning, Rewiring):** Mechanisms allowing FUM to physically alter its network structure by adding neurons/connections (Growth), removing them (Pruning), or changing existing connections (Rewiring), triggered by metrics like cluster reward, neuron activity, or connection efficacy. (See Section 1.A.2, 1.C.5, 4.C, 5.B.2, 5.E.4, 6.A.4)
*   **Supervised Learning:** A machine learning paradigm requiring labeled data (input-output pairs) for training, contrasted with FUM's reliance on reinforcement learning (SIE) and unsupervised aspects (STDP). (See Section 2.C.1)
*   **Synaptic Scaling:** A homeostatic mechanism that adjusts the overall strength of excitatory inputs to a neuron to keep its activity within a stable range, preventing saturation or silence. FUM applies scaling periodically, potentially protecting recently potentiated synapses. (See Section 2.B.7, 5.E.4, 5.E.7)
*   **Synchronization:** The process of coordinating the timing of operations across different parts of the distributed FUM system, managed using mechanisms like PTP, vector clocks, and periodic global barriers, crucial for maintaining consistency and STDP accuracy. (See Section 2.E.3, 5.D.2)
*   **Tandem Complexity Scaling (Phase 2):** The second phase of FUM training, focused on refining the network structure and achieving baseline competence by training on a curated curriculum of increasing complexity (up to 300 inputs). (See Section 5.B)
*   **TD (Temporal Difference) Learning / TD Error / TD(0):** A reinforcement learning method used within FUM's SIE. TD error (`r + γ * V(next_state) - V(current_state)`) estimates the difference between predicted future reward (`V(state)`) and actual reward plus discounted future reward, driving updates to the value function. FUM uses TD(0). (See Section 1.C.4, 2.C.3, 5.B.3)
*   **Temporal Coding / Decoding:** Representing information using the precise timing of spikes, not just their average rate. FUM utilizes temporal aspects in encoding, SNN dynamics (STDP), and potentially decoding. (See Section 1.B.3, 3.A.2, 3.B.2, 6.A.3)
*   **Temporal Credit Assignment:** The challenge in reinforcement learning of assigning rewards or penalties to the specific past actions or events (like spike timings) that contributed to the outcome, especially when there's a delay. FUM uses eligibility traces (potentially with STC enhancements) for this. (See Section 2.B.5)
*   **Tensor-Based Computation:** Utilizing libraries like PyTorch for efficient operations on multi-dimensional arrays (tensors), employed in FUM's hybrid architecture for tasks like SIE calculation, clustering, etc., complementing the SNN simulation. (See Section 1.B.5, 2.E)
*   **Thermodynamic Intelligence Model / Thermodynamic Models of Cognition:** A theoretical framework applied to FUM, modeling emergent intelligence as potentially analogous to a thermodynamic system, where complexity drives phase transitions to higher-order capabilities, correlating with metrics like reasoning depth. (See Section 4.K.2)
*   **Tokenization:** The process used by many NLP models (like LLMs) to break input text into smaller units (tokens, often sub-words) before converting them into numerical representations (embeddings). Contrasted with FUM's spike-based encoding. (See Section 3.A.1)
*   **Unified (Model Philosophy):** Refers to FUM's core design principle of integrating diverse computational paradigms (SNNs, reinforcement learning via SIE, unsupervised learning via STDP, structural plasticity) and mechanisms into a single, cohesive, self-improving system capable of processing multimodal inputs and generating complex behaviors. (See Section 1.B.1)
*   **Unified Debugging Framework:** An integrated approach in FUM combining Spike Pathway Tracing, the Causal Inference Engine, and the Predictive Debugging Model into a streamlined system for identifying and diagnosing emergent failures at scale, validated to achieve high accuracy (e.g., 99% @ 5B neurons) with reduced overhead. (See Section 5.E.11)
*   **Unified Knowledge Graph:** See Emergent Knowledge Graph.
*   **Validation (Emergent / Brain-Inspired):** FUM's approach to testing generalization and robustness, prioritizing performance on diverse synthetic data generated by the system itself and curated real-world examples, rather than solely optimizing for standard benchmarks or relying on massive internet-scale test sets. Includes specific tests like OOD and Junk Data Injection. (See Section 1.A.3, 6.A.7, 6.A.9)
*   **Value Function (`V(state)`):** In reinforcement learning (specifically TD learning in FUM's SIE), a function that estimates the expected future cumulative reward starting from a given state. In FUM, states are typically represented by cluster IDs. (See Section 2.C.3, 2.F.1)
*   **Vector Clock:** A mechanism used in distributed systems to track causal dependencies between events occurring on different nodes, employed in FUM to ensure conflict-free updates during asynchronous operation. (See Section 5.D.2)
